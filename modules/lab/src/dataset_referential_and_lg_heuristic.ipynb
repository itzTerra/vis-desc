{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899d3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from numpy.typing import NDArray\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import ZODB\n",
    "import ZODB.FileStorage\n",
    "import transaction\n",
    "from text2features import FeatureExtractorPipeline, ExtCtx, SentenceToken\n",
    "from text2features_paths import FEATURE_PIPELINE_RESOURCES\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from book_segmenting import TextSegmenter\n",
    "from utils import DATA_DIR\n",
    "\n",
    "feature_extractor = FeatureExtractorPipeline(resources=FEATURE_PIPELINE_RESOURCES)\n",
    "\n",
    "SEGMENT_CHARS_MIN = 150\n",
    "SEGMENT_CHARS_MAX = 500\n",
    "segmenter = TextSegmenter(segment_size=(SEGMENT_CHARS_MIN, SEGMENT_CHARS_MAX))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    MIN_TEXT_LENGTH = 60\n",
    "    MAX_TEXT_LENGTH = 500\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src: Iterable[Any],\n",
    "        take: int,\n",
    "        skip: int = 0,\n",
    "        text_getter=None,\n",
    "        deduplicate=False,\n",
    "        segment=False,\n",
    "        check_length=True,\n",
    "    ):\n",
    "        self.src = iter(src)\n",
    "        self.take = take\n",
    "        self.skip = skip\n",
    "        self.contexts: list[ExtCtx] | None = None\n",
    "        self.features: list[NDArray[np.float32]] | None = None\n",
    "        self.text_getter = text_getter\n",
    "        self.deduplicate = deduplicate\n",
    "        self.segment = segment\n",
    "        self.check_length = check_length\n",
    "\n",
    "    def process(\n",
    "        self, deduplicate: bool | None = None, segment: bool | None = None\n",
    "    ) -> list[ExtCtx]:\n",
    "        if deduplicate is None:\n",
    "            deduplicate = self.deduplicate\n",
    "        if segment is None:\n",
    "            segment = self.segment\n",
    "\n",
    "        self.contexts = []\n",
    "        self.features = []\n",
    "        if deduplicate:\n",
    "            seen = set()\n",
    "        taken = 0\n",
    "        to_skip = self.skip\n",
    "\n",
    "        with tqdm(total=self.take, desc=\"Processing texts\", unit=\"text\") as pbar:\n",
    "            while taken < self.take:\n",
    "                try:\n",
    "                    text = next(self.src)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                if to_skip > 0:\n",
    "                    to_skip -= 1\n",
    "                    continue\n",
    "\n",
    "                if self.text_getter is not None:\n",
    "                    text = self.text_getter(text)\n",
    "                if not text or (\n",
    "                    self.check_length and len(text.strip()) < Dataset.MIN_TEXT_LENGTH\n",
    "                ):\n",
    "                    continue\n",
    "                if deduplicate:\n",
    "                    if text in seen:\n",
    "                        continue\n",
    "                    seen.add(text)\n",
    "\n",
    "                text = FeatureExtractorPipeline.preprocess(text)\n",
    "                if self.check_length and len(text.strip()) < Dataset.MIN_TEXT_LENGTH:\n",
    "                    continue\n",
    "\n",
    "                if segment:\n",
    "                    segments = [\n",
    "                        seg\n",
    "                        for seg in segmenter.segment_text(text)\n",
    "                        if seg\n",
    "                        and (\n",
    "                            not self.check_length\n",
    "                            or (seg_len := len(seg.strip())) >= Dataset.MIN_TEXT_LENGTH\n",
    "                            and seg_len <= Dataset.MAX_TEXT_LENGTH\n",
    "                        )\n",
    "                    ]\n",
    "                    if len(segments) == 0:\n",
    "                        continue\n",
    "                    example = segments[len(segments) // 2]\n",
    "                    ctx = feature_extractor.get_ctx(example)\n",
    "                    self.contexts.append(ctx)\n",
    "                    self.features.append(\n",
    "                        feature_extractor.extract(example, preprocess=False, ctx=ctx)\n",
    "                    )\n",
    "                else:\n",
    "                    if self.check_length and len(text) > Dataset.MAX_TEXT_LENGTH:\n",
    "                        continue\n",
    "                    ctx = feature_extractor.get_ctx(text)\n",
    "                    self.contexts.append(ctx)\n",
    "                    self.features.append(\n",
    "                        feature_extractor.extract(text, preprocess=False, ctx=ctx)\n",
    "                    )\n",
    "                taken += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        return self.contexts\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.contexts is None:\n",
    "            raise ValueError(\"Dataset not processed yet. Call process() first.\")\n",
    "        return iter(self.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18499766",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_high_flickr = load_dataset(\n",
    "    \"CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c10991",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_flickr30k = load_dataset(\"embedding-data/flickr30k_captions_quintets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2a2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_coco = load_dataset(\"sentence-transformers/coco-captions\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ffb2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sbu = load_dataset(\"vicenteor/sbu_captions\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392b464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"movie_summaries.txt\") as f:\n",
    "    ds_movie_summaries = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d47a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_book_summaries = load_dataset(\"textminr/cmu-book-summaries\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343f730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"book_dialogs.txt\") as f:\n",
    "    ds_book_dialogs = [line.strip() for line in f.read().split(\"\\n\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95984a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b8301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_news = load_dataset(\"EdinburghNLP/xsum\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07bd44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hotels = load_dataset(\"argilla/tripadvisor-hotel-reviews\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f04a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_yelp = load_dataset(\"Yelp/yelp_review_full\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b5d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_arxiv = load_dataset(\n",
    "    \"armanc/scientific_papers\",\n",
    "    \"arxiv\",\n",
    "    split=\"validation\",\n",
    "    trust_remote_code=True,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac74e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMAZON_CATEGORIES = [\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Beauty_and_Personal_Care\",\n",
    "    \"Electronics\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"CDs_and_Vinyl\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Software\",\n",
    "]\n",
    "ds_amazon_reviews = []\n",
    "N_TOTAL = 2000\n",
    "for category in AMAZON_CATEGORIES:\n",
    "    ds = iter(\n",
    "        load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{category}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "            streaming=True,\n",
    "        )\n",
    "    )\n",
    "    for i in range(N_TOTAL // len(AMAZON_CATEGORIES)):\n",
    "        ds_amazon_reviews.append(next(ds))\n",
    "while len(ds_amazon_reviews) < N_TOTAL:\n",
    "    ds_amazon_reviews.append(next(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25eaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"batch_10k.json\") as f:\n",
    "    books_10k_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 0/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1500/1500 [04:01<00:00,  6.21text/s]\n",
      "Processing texts: 100%|██████████| 1500/1500 [04:01<00:00,  6.21text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1500/1500 [02:22<00:00, 10.53text/s]\n",
      "Processing texts: 100%|██████████| 1500/1500 [02:22<00:00, 10.53text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1500/1500 [00:58<00:00, 25.48text/s]\n",
      "Processing texts: 100%|██████████| 1500/1500 [00:58<00:00, 25.48text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 3/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:17<00:00, 29.16text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:17<00:00, 29.16text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 4/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:19<00:00, 25.16text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:19<00:00, 25.16text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 5/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:31<00:00, 16.06text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:31<00:00, 16.06text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 6/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:32<00:00, 15.38text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:32<00:00, 15.38text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 7/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:48<00:00, 10.20text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:48<00:00, 10.20text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 8/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1000/1000 [01:19<00:00, 12.52text/s]\n",
      "Processing texts: 100%|██████████| 1000/1000 [01:19<00:00, 12.52text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 9/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:34<00:00, 14.41text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:34<00:00, 14.41text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 10/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 200/200 [00:15<00:00, 12.68text/s]\n",
      "Processing texts: 100%|██████████| 200/200 [00:15<00:00, 12.68text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 11/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 300/300 [00:27<00:00, 10.81text/s]\n",
      "Processing texts: 100%|██████████| 300/300 [00:27<00:00, 10.81text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 12/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:   0%|          | 0/500 [05:04<?, ?text/s]\n",
      "\n"
     ]
    },
    {
     "ename": "FSTimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/aiohttp/streams.py:347\u001b[39m, in \u001b[36mStreamReader._wait\u001b[39m\u001b[34m(self, func_name)\u001b[39m\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._timer:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[39m, in \u001b[36m_runner\u001b[39m\u001b[34m(event, coro, result, timeout)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     result[\u001b[32m0\u001b[39m] = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/implementations/http.py:672\u001b[39m, in \u001b[36mHTTPFile.async_fetch_range\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_is_range:\n\u001b[32m    671\u001b[39m     \u001b[38;5;66;03m# partial content, as expected\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m     out = \u001b[38;5;28;01mawait\u001b[39;00m r.read()\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m start > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/aiohttp/client_reqrep.py:1218\u001b[39m, in \u001b[36mClientResponse.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m     \u001b[38;5;28mself\u001b[39m._body = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.content.read()\n\u001b[32m   1219\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._traces:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/aiohttp/streams.py:418\u001b[39m, in \u001b[36mStreamReader.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     block = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.readany()\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/aiohttp/streams.py:440\u001b[39m, in \u001b[36mStreamReader.readany\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait(\u001b[33m\"\u001b[39m\u001b[33mreadany\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_nowait(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/aiohttp/streams.py:346\u001b[39m, in \u001b[36mStreamReader._wait\u001b[39m\u001b[34m(self, func_name)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaiter\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/aiohttp/helpers.py:671\u001b[39m, in \u001b[36mTimerContext.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    670\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio.TimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_val\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFSTimeoutError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(datasets):\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Done: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(datasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeduplicate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDONE\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mDataset.process\u001b[39m\u001b[34m(self, deduplicate, segment)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m taken < \u001b[38;5;28mself\u001b[39m.take:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         text = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/datasets/iterable_dataset.py:2226\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2226\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_typed\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;49;00m\n\u001b[32m   2229\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;49;00m\n\u001b[32m   2230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_apply_feature_types_on_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_token_per_repo_id\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/datasets/iterable_dataset.py:219\u001b[39m, in \u001b[36mExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gen_kwags \u001b[38;5;129;01min\u001b[39;00m islice(_split_gen_kwargs(\u001b[38;5;28mself\u001b[39m.kwargs, max_num_jobs=\u001b[38;5;28mself\u001b[39m.num_shards), shard_idx_start, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    218\u001b[39m     shard_example_idx_start = \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mshard_example_idx\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_examples_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshard_example_idx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/datasets_modules/datasets/armanc--scientific_papers/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f/scientific_papers.py:120\u001b[39m, in \u001b[36mScientificPapers._generate_examples\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    119\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Yields examples.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    121\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m    122\u001b[39m             \u001b[38;5;66;03m# Possible keys are:\u001b[39;00m\n\u001b[32m    123\u001b[39m             \u001b[38;5;66;03m# \"article_id\": str\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m             \u001b[38;5;66;03m# \"section_names\": list[str], list of section names.\u001b[39;00m\n\u001b[32m    127\u001b[39m             \u001b[38;5;66;03m# \"sections\": list[list[str]], list of sections (list of paragraphs)\u001b[39;00m\n\u001b[32m    128\u001b[39m             d = json.loads(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/datasets/streaming.py:75\u001b[39m, in \u001b[36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:948\u001b[39m, in \u001b[36mxopen\u001b[39m\u001b[34m(file, mode, download_config, *args, **kwargs)\u001b[39m\n\u001b[32m    946\u001b[39m kwargs = {**kwargs, **(storage_options \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     file_obj = \u001b[43mfsspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot seek streaming HTTP file\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/core.py:147\u001b[39m, in \u001b[36mOpenFile.open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/core.py:105\u001b[39m, in \u001b[36mOpenFile.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode.replace(\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m.path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/spec.py:1310\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1309\u001b[39m     ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1319\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/implementations/zip.py:130\u001b[39m, in \u001b[36mZipFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, autocommit, cache_options, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mZipFS can only be open for reading or writing, not both\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_zip64\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforce_zip_64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    132\u001b[39m     info = \u001b[38;5;28mself\u001b[39m.info(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1620\u001b[39m, in \u001b[36mZipFile.open\u001b[39m\u001b[34m(self, name, mode, pwd, force_zip64)\u001b[39m\n\u001b[32m   1616\u001b[39m zef_file = _SharedFile(\u001b[38;5;28mself\u001b[39m.fp, zinfo.header_offset,\n\u001b[32m   1617\u001b[39m                        \u001b[38;5;28mself\u001b[39m._fpclose, \u001b[38;5;28mself\u001b[39m._lock, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._writing)\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1619\u001b[39m     \u001b[38;5;66;03m# Skip the file header:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     fheader = \u001b[43mzef_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizeFileHeader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fheader) != sizeFileHeader:\n\u001b[32m   1622\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mTruncated file header\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:808\u001b[39m, in \u001b[36m_SharedFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt read from the ZIP file while there \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    805\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis an open writing handle on it. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    806\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mClose the writing handle before trying to read.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    807\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[38;5;28mself\u001b[39m._pos)\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;28mself\u001b[39m._pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/implementations/http.py:603\u001b[39m, in \u001b[36mHTTPFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    602\u001b[39m     length = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.size - \u001b[38;5;28mself\u001b[39m.loc, length)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/spec.py:2083\u001b[39m, in \u001b[36mAbstractBufferedFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m length == \u001b[32m0\u001b[39m:\n\u001b[32m   2081\u001b[39m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[32m   2082\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2085\u001b[39m logger.debug(\n\u001b[32m   2086\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2087\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2090\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache._log_stats(),\n\u001b[32m   2091\u001b[39m )\n\u001b[32m   2092\u001b[39m \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/caching.py:506\u001b[39m, in \u001b[36mBytesCache._fetch\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.end - end > \u001b[38;5;28mself\u001b[39m.blocksize:\n\u001b[32m    505\u001b[39m     \u001b[38;5;28mself\u001b[39m.total_requested_bytes += bend - start\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m.start = start\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[39m, in \u001b[36msync_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m = obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/fsspec/asyn.py:101\u001b[39m, in \u001b[36msync\u001b[39m\u001b[34m(loop, func, timeout, *args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m return_result = result[\u001b[32m0\u001b[39m]\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, asyncio.TimeoutError):\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# suppress asyncio.TimeoutError, raise FSTimeoutError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreturn_result\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n",
      "\u001b[31mFSTimeoutError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    Dataset(ds_high_flickr, take=1500, text_getter=lambda x: x[\"caption_internlm2\"]),\n",
    "    Dataset(\n",
    "        ds_high_flickr, take=1500, text_getter=lambda x: x[\"caption_internlm2_short\"]\n",
    "    ),\n",
    "    Dataset(ds_flickr30k, take=1500, text_getter=lambda x: x[\"set\"][0]),\n",
    "    Dataset(ds_coco, take=500, text_getter=lambda x: x[\"caption1\"]),\n",
    "    Dataset(ds_sbu, take=500, text_getter=lambda x: x[\"caption\"]),\n",
    "    Dataset(ds_movie_summaries, take=500, segment=True),\n",
    "    Dataset(\n",
    "        ds_book_summaries, take=500, text_getter=lambda x: x[\"summary\"], segment=True\n",
    "    ),\n",
    "    Dataset(ds_book_dialogs, take=500),\n",
    "    Dataset(\n",
    "        ds_wiki,\n",
    "        take=1000,\n",
    "        text_getter=lambda x: x[\"text\"].replace(\" @-@ \", \"-\").replace(\" @,@ \", \",\"),\n",
    "        segment=True,\n",
    "    ),\n",
    "    Dataset(ds_news, take=500, text_getter=lambda x: x[\"document\"], segment=True),\n",
    "    Dataset(ds_hotels, take=200, text_getter=lambda x: x[\"text\"]),\n",
    "    Dataset(ds_yelp, take=300, text_getter=lambda x: x[\"text\"]),\n",
    "    Dataset(ds_arxiv, take=500, text_getter=lambda x: x[\"abstract\"], segment=True),\n",
    "    Dataset(ds_amazon_reviews, take=500, text_getter=lambda x: x[\"text\"]),\n",
    "    Dataset(\n",
    "        books_10k_dataset,\n",
    "        take=10000,\n",
    "        text_getter=lambda x: x[\"text\"],\n",
    "        check_length=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f\"--- Done: {i}/{len(datasets)} ---\")\n",
    "    dataset.process(deduplicate=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a24474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 0/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:   0%|          | 0/500 [00:00<?, ?text/s]\n",
      "Processing texts:   0%|          | 0/500 [00:00<?, ?text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:28<00:00, 17.37text/s]\n",
      "Processing texts: 100%|██████████| 500/500 [00:28<00:00, 17.37text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 10000/10000 [13:22<00:00, 12.46text/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(datasets[12:]):\n",
    "    print(f\"--- Done: {i}/{len(datasets)} ---\")\n",
    "    dataset.process(deduplicate=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07597623",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = ZODB.FileStorage.FileStorage(\"../data/db/mydata.fs\")\n",
    "db = ZODB.DB(storage)\n",
    "connection = db.open()\n",
    "root = connection.root\n",
    "\n",
    "# root.datasets = BTrees.OOBTree.BTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZODB Load\n",
    "datasets_features: list[NDArray[np.float32]] = root.datasets[\"ref_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647f3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/ZODB/Connection.py:574: UserWarning: The <class 'BTrees.OOBTree.OOBTree'>\n",
      "object you're saving is large. (1622460330 bytes.)\n",
      "\n",
      "Perhaps you're storing media which should be stored in blobs.\n",
      "\n",
      "Perhaps you're using a non-scalable data structure, such as a\n",
      "PersistentMapping or PersistentList.\n",
      "\n",
      "Perhaps you're storing data in objects that aren't persistent at\n",
      "all. In cases like that, the data is stored in the record of the\n",
      "containing persistent object.\n",
      "\n",
      "In any case, storing records this big is probably a bad idea.\n",
      "\n",
      "If you insist and want to get rid of this warning, use the\n",
      "large_record_size option of the ZODB.DB constructor (or the\n",
      "large-record-size option in a configuration file) to specify a larger\n",
      "size.\n",
      "\n",
      "  warnings.warn(large_object_message % (obj.__class__, len(p)))\n"
     ]
    }
   ],
   "source": [
    "# ZODB Save\n",
    "def save_datasets(datasets: list[Dataset], name: str):\n",
    "    root.datasets[f\"{name}_contexts\"] = [ds.contexts for ds in datasets]\n",
    "    root.datasets[f\"{name}_features\"] = [ds.features for ds in datasets]\n",
    "    transaction.commit()\n",
    "\n",
    "\n",
    "save_datasets(datasets, name=\"ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "\n",
    "def extract_char_ngrams(ctx: ExtCtx, ctr: Counter, document_ctr: Counter) -> Counter:\n",
    "    \"\"\"Required ctx: 'text'\n",
    "\n",
    "    Time complexity: O(mn), where m is number of characters and n is n-gram size\n",
    "    \"\"\"\n",
    "    MAX_N = 5\n",
    "\n",
    "    document_set = set()\n",
    "    text = ctx.text.casefold()\n",
    "    for n in range(2, MAX_N + 1):\n",
    "        for i in range(len(text) - n + 1):\n",
    "            ngram = text[i : i + n]\n",
    "            ctr[ngram] += 1\n",
    "            document_set.add(ngram)\n",
    "    document_ctr.update(document_set)\n",
    "\n",
    "\n",
    "def extract_pos_ngrams(ctx: ExtCtx, ctr: Counter, document_ctr: Counter) -> Counter:\n",
    "    \"\"\"Required ctx: 'tokens'\n",
    "\n",
    "    Time complexity: O(mn), where m is number of tokens and n is n-gram size\n",
    "\n",
    "    Uses coarse UD tags:\n",
    "    ADJ: adjective\n",
    "    ADP: adposition\n",
    "    ADV: adverb\n",
    "    AUX: auxiliary\n",
    "    CCONJ: coordinating conjunction\n",
    "    DET: determiner\n",
    "    INTJ: interjection\n",
    "    NOUN: noun\n",
    "    NUM: numeral\n",
    "    PART: particle\n",
    "    PRON: pronoun\n",
    "    PROPN: proper noun\n",
    "    PUNCT: punctuation\n",
    "    SCONJ: subordinating conjunction\n",
    "    SYM: symbol\n",
    "    VERB: verb\n",
    "    X: other\n",
    "    \"\"\"\n",
    "    MAX_N = 4\n",
    "\n",
    "    document_set = set()\n",
    "    tokens = ctx.tokens\n",
    "    for n in range(2, MAX_N + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(token.pos for token in tokens[i : i + n])\n",
    "            ctr[ngram] += 1\n",
    "            document_set.add(ngram)\n",
    "    document_ctr.update(document_set)\n",
    "\n",
    "\n",
    "def extract_dependency_tree_structure(\n",
    "    ctx: ExtCtx, depth_ctr: Counter, branching_factor_ctr: Counter, width_ctr: Counter\n",
    "):\n",
    "    \"\"\"Required ctx: 'sents'\n",
    "\n",
    "    Complexity: O(n), where n is number of tokens in all sentences\n",
    "    \"\"\"\n",
    "\n",
    "    def get_tree_depth(token: SentenceToken):\n",
    "        \"\"\"Compute the longest path from root to any leaf node.\"\"\"\n",
    "        if not list(token.children):  # leaf node\n",
    "            return 0\n",
    "        return 1 + max(get_tree_depth(child) for child in token.children)\n",
    "\n",
    "    def get_branching_factors(token: SentenceToken, factors=None):\n",
    "        \"\"\"Get branching factor for each non-leaf node.\"\"\"\n",
    "        if factors is None:\n",
    "            factors = []\n",
    "\n",
    "        children = list(token.children)\n",
    "        if children:\n",
    "            factors.append(len(children))\n",
    "            for child in children:\n",
    "                get_branching_factors(child, factors)\n",
    "\n",
    "        return factors\n",
    "\n",
    "    def count_leaf_nodes(token: SentenceToken):\n",
    "        \"\"\"Count the number of leaf nodes in the tree.\"\"\"\n",
    "        children = list(token.children)\n",
    "        if not children:\n",
    "            return 1\n",
    "        return sum(count_leaf_nodes(child) for child in children)\n",
    "\n",
    "    for sent in ctx.sents:\n",
    "        root = sent.root\n",
    "\n",
    "        depth = get_tree_depth(root)\n",
    "        depth_ctr[depth] += 1\n",
    "\n",
    "        factors = get_branching_factors(root)\n",
    "        branching_factor_ctr.update(factors)\n",
    "\n",
    "        width = count_leaf_nodes(root)\n",
    "        width_ctr[width] += 1\n",
    "\n",
    "\n",
    "def extract_dependency_tree_relations(\n",
    "    ctx: ExtCtx,\n",
    "    node_ngrams: Counter,\n",
    "    relation_ngrams: Counter,\n",
    "    complete_ngrams: Counter,\n",
    "    node_doc_freq: Counter,\n",
    "    relation_doc_freq: Counter,\n",
    "    complete_doc_freq: Counter,\n",
    "):\n",
    "    \"\"\"Required ctx: 'sents'\n",
    "\n",
    "    Returns three counters:\n",
    "    1. Node n-grams (2-4-grams) - ascending path of node labels (POS tags)\n",
    "    2. Relation n-grams (1-4-grams) - ascending path of edge labels (dependency relations)\n",
    "    3. Complete n-grams (2-4-grams) - path with both node and edge labels\n",
    "    \"\"\"\n",
    "    node_ngrams_set = set()\n",
    "    relation_ngrams_set = set()\n",
    "    complete_ngrams_set = set()\n",
    "\n",
    "    def get_ascending_paths(\n",
    "        token: SentenceToken,\n",
    "        current_path_nodes=None,\n",
    "        current_path_rels=None,\n",
    "        visited=None,\n",
    "    ):\n",
    "        \"\"\"Get all ascending paths starting from this token\"\"\"\n",
    "        if current_path_nodes is None:\n",
    "            current_path_nodes = []\n",
    "        if current_path_rels is None:\n",
    "            current_path_rels = []\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "\n",
    "        if token.text in visited:  # Avoid cycles\n",
    "            return\n",
    "\n",
    "        visited.add(token.text)\n",
    "        current_path_nodes.append(token.pos)\n",
    "\n",
    "        # Process current path for node n-grams (2-4)\n",
    "        path_len = len(current_path_nodes)\n",
    "        for n in range(2, min(5, path_len + 1)):\n",
    "            if path_len >= n:\n",
    "                ngram = tuple(current_path_nodes[-n:])\n",
    "                node_ngrams[ngram] += 1\n",
    "                node_ngrams_set.add(ngram)\n",
    "\n",
    "        # Process relation n-grams (1-4)\n",
    "        if current_path_rels:\n",
    "            for n in range(1, min(5, len(current_path_rels) + 1)):\n",
    "                if len(current_path_rels) >= n:\n",
    "                    ngram = tuple(current_path_rels[-n:])\n",
    "                    relation_ngrams[ngram] += 1\n",
    "                    relation_ngrams_set.add(ngram)\n",
    "\n",
    "        # Process complete n-grams (2-4) - alternating node-rel-node\n",
    "        if len(current_path_nodes) >= 2 and len(current_path_rels) >= 1:\n",
    "            for n in range(2, min(5, len(current_path_nodes) + 1)):\n",
    "                if len(current_path_nodes) >= n and len(current_path_rels) >= n - 1:\n",
    "                    complete_path = []\n",
    "                    for i in range(n):\n",
    "                        complete_path.append(current_path_nodes[-(n - i)])\n",
    "                        if i < n - 1 and len(current_path_rels) > (n - 2 - i):\n",
    "                            complete_path.append(current_path_rels[-(n - 1 - i)])\n",
    "                    complete_ngrams[tuple(complete_path)] += 1\n",
    "                    complete_ngrams_set.add(tuple(complete_path))\n",
    "\n",
    "        for child in token.children:\n",
    "            new_path_rels = current_path_rels + [child.dep]\n",
    "            get_ascending_paths(\n",
    "                child, current_path_nodes[:], new_path_rels[:], visited.copy()\n",
    "            )\n",
    "\n",
    "        visited.remove(token.text)\n",
    "\n",
    "    for sent in ctx.sents:\n",
    "        get_ascending_paths(sent.root)\n",
    "\n",
    "    node_doc_freq.update(node_ngrams_set)\n",
    "    relation_doc_freq.update(relation_ngrams_set)\n",
    "    complete_doc_freq.update(complete_ngrams_set)\n",
    "\n",
    "    return node_ngrams, relation_ngrams, complete_ngrams\n",
    "\n",
    "\n",
    "def extract_noun_phrase_lengths(ctx: ExtCtx, np_length_ctr: Counter):\n",
    "    \"\"\"Required ctx: 'noun_chunks'\n",
    "\n",
    "    Time complexity: O(m), where m is number of noun phrases\n",
    "    \"\"\"\n",
    "    for chunk in ctx.noun_chunks:\n",
    "        np_length_ctr[chunk.length] += 1\n",
    "\n",
    "\n",
    "prepositions_path = DATA_DIR / \"datasets\" / \"concreteness\" / \"prepositions.csv\"\n",
    "prep_imag_data = pd.read_csv(prepositions_path, na_values=[\"NA\"])\n",
    "_compiled_regex_patterns = {}\n",
    "_exact_match_patterns = {}\n",
    "_multi_word_patterns = {}\n",
    "for _, row in prep_imag_data.iterrows():\n",
    "    prep = row[\"prep\"]\n",
    "    imag = row[\"imag\"]\n",
    "\n",
    "    if row[\"is_regex\"] == 1:\n",
    "        # Compile regex patterns\n",
    "        _compiled_regex_patterns[prep] = (\n",
    "            regex.compile(prep, regex.IGNORECASE),\n",
    "            imag,\n",
    "        )\n",
    "    elif row[\"n_words\"] > 1:\n",
    "        # Multi-word exact matches\n",
    "        _multi_word_patterns[prep.casefold()] = imag\n",
    "    else:\n",
    "        # Single word exact matches\n",
    "        _exact_match_patterns[prep.casefold()] = (\n",
    "            imag,\n",
    "            row[\"pos_adp\"],\n",
    "            row[\"pos_nonadp\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def extract_preposition_imageability(\n",
    "    ctx: ExtCtx,\n",
    ") -> tuple[NDArray[np.float32], int, int]:\n",
    "    \"\"\"Required ctx: 'tokens', 'words'\n",
    "\n",
    "    Time complexity: O(mk + n), where m is number of words, k is average multi-word phrase length, and n is number of matched prepositions\n",
    "\n",
    "    Returns: Tuple of (features, match_count, effective_word_count) where:\n",
    "    - features: (BINS+1)-dimensional vector where first BINS dimensions are histogram frequencies\n",
    "      and the last dimension is the average imageability value.\n",
    "    - match_count: number of matched prepositions (counting multi-word phrases as single matches)\n",
    "    - effective_word_count: total word count adjusted for multi-word phrases\n",
    "    \"\"\"\n",
    "    BINS = 10\n",
    "\n",
    "    tokens = ctx.tokens\n",
    "    if not tokens or not ctx.words:\n",
    "        return (\n",
    "            np.zeros(BINS + 1, dtype=np.float32),\n",
    "            0,\n",
    "            len(ctx.words) if ctx.words else 0,\n",
    "        )\n",
    "\n",
    "    imageability_values = []\n",
    "    matched_positions = set()\n",
    "\n",
    "    # Multiword\n",
    "    for prep_phrase, imag_val in _multi_word_patterns.items():\n",
    "        phrase_words = prep_phrase.split()\n",
    "        phrase_len = len(phrase_words)\n",
    "\n",
    "        for i in range(len(tokens) - phrase_len + 1):\n",
    "            if i in matched_positions:\n",
    "                continue\n",
    "            if all(tokens[i + j].itext == phrase_words[j] for j in range(phrase_len)):\n",
    "                if not any(\n",
    "                    pos in matched_positions for pos in range(i, i + phrase_len)\n",
    "                ):\n",
    "                    imageability_values.append(imag_val)\n",
    "                    matched_positions.update(range(i, i + phrase_len))\n",
    "\n",
    "    # Singleword regex\n",
    "    for compiled_pattern, imag_val in _compiled_regex_patterns.values():\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if i in matched_positions:\n",
    "                continue\n",
    "            if compiled_pattern.search(tok.itext):\n",
    "                imageability_values.append(imag_val)\n",
    "                matched_positions.add(i)\n",
    "\n",
    "    # Singleword\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i in matched_positions:\n",
    "            continue\n",
    "\n",
    "        matched = (\n",
    "            token.itext\n",
    "            if token.itext in _exact_match_patterns\n",
    "            else (token.lemma if token.lemma in _exact_match_patterns else None)\n",
    "        )\n",
    "\n",
    "        if matched is not None:\n",
    "            val, pos_adp, pos_nonadp = _exact_match_patterns[matched]\n",
    "            final_val = None\n",
    "\n",
    "            if token.pos == \"ADP\" and not np.isnan(pos_adp):\n",
    "                final_val = pos_adp\n",
    "            elif token.pos != \"ADP\" and not np.isnan(pos_nonadp):\n",
    "                final_val = pos_nonadp\n",
    "            elif np.isnan(pos_adp) and np.isnan(pos_nonadp):\n",
    "                final_val = val\n",
    "\n",
    "            if final_val is not None:\n",
    "                imageability_values.append(final_val)\n",
    "                matched_positions.add(i)\n",
    "\n",
    "    match_count = len(imageability_values)\n",
    "    # Calculate effective word count: original count minus multi-word reductions\n",
    "    # Each multi-word match reduces the count by (phrase_length - 1)\n",
    "    multiword_reduction = len(matched_positions) - match_count\n",
    "    effective_word_count = (\n",
    "        sum(1 for token in ctx.tokens if token.pos == \"ADP\") - multiword_reduction\n",
    "    )\n",
    "\n",
    "    if not imageability_values:\n",
    "        return np.zeros(BINS + 1, dtype=np.float32), 0, effective_word_count\n",
    "\n",
    "    hist_counts = np.zeros(BINS, dtype=np.float32)\n",
    "    total = 0.0\n",
    "    for imag_val in imageability_values:\n",
    "        bin_idx = min(int(imag_val * BINS), BINS - 1)\n",
    "        hist_counts[bin_idx] += 1\n",
    "        total += imag_val\n",
    "    hist_freqs = hist_counts / len(ctx.words)\n",
    "    avg_imageability = total / len(imageability_values)\n",
    "\n",
    "    return (\n",
    "        np.concatenate([hist_freqs, np.array([avg_imageability], dtype=np.float32)]),\n",
    "        match_count,\n",
    "        effective_word_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae94a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter = Counter()\n",
    "char_doc_freq = Counter()\n",
    "pos_counter = Counter()\n",
    "pos_doc_freq = Counter()\n",
    "dep_tree_depth_counter = Counter()\n",
    "dep_tree_branching_factor_counter = Counter()\n",
    "dep_tree_width_counter = Counter()\n",
    "node_ngrams = Counter()  # 2-4-grams of POS tags\n",
    "node_doc_freq = Counter()\n",
    "relation_ngrams = Counter()  # 1-4-grams of dependency relations\n",
    "relation_doc_freq = Counter()\n",
    "complete_ngrams = Counter()  # 2-4-grams of alternating POS-rel-POS patterns\n",
    "complete_doc_freq = Counter()\n",
    "np_length_ctr = Counter()\n",
    "concr_matches = 0\n",
    "concr_effective_word_count = 0\n",
    "prep_matches = 0\n",
    "prep_effective_word_count = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    for ctx in dataset:\n",
    "        # extract_char_ngrams(ctx, char_counter, char_doc_freq)\n",
    "        # extract_pos_ngrams(ctx, pos_counter, pos_doc_freq)\n",
    "        # extract_dependency_tree_structure(\n",
    "        #     ctx,\n",
    "        #     dep_tree_depth_counter,\n",
    "        #     dep_tree_branching_factor_counter,\n",
    "        #     dep_tree_width_counter,\n",
    "        # )\n",
    "        # extract_dependency_tree_relations(\n",
    "        #     ctx,\n",
    "        #     node_ngrams,\n",
    "        #     relation_ngrams,\n",
    "        #     complete_ngrams,\n",
    "        #     node_doc_freq,\n",
    "        #     relation_doc_freq,\n",
    "        #     complete_doc_freq,\n",
    "        # )\n",
    "        # extract_noun_phrase_lengths(ctx, np_length_ctr)\n",
    "        # _, _, match_count, effective_word_count = (\n",
    "        #     feature_extractor.extract_word_concreteness(ctx)\n",
    "        # )\n",
    "        # concr_matches += match_count\n",
    "        # concr_effective_word_count += effective_word_count\n",
    "\n",
    "        _, prep_match_count, prep_effective_wc = extract_preposition_imageability(ctx)\n",
    "        prep_matches += prep_match_count\n",
    "        prep_effective_word_count += prep_effective_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab585a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_example_count = sum(ds.take for ds in datasets)\n",
    "\n",
    "# Get top 1000 ngrams closest to 50% document frequency in each dataset\n",
    "df1 = pd.DataFrame(char_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"])\n",
    "df1[\"doc_freq_ratio\"] = df1[\"doc_freq\"] / total_example_count\n",
    "df1[\"doc_freq_diff\"] = np.abs(df1[\"doc_freq_ratio\"] - 0.5)\n",
    "df1 = df1.sort_values(\"doc_freq_diff\").reset_index(drop=True)\n",
    "df1 = df1.head(1000)\n",
    "# df1.to_csv(\"char_ngrams_features.csv\", index=False)\n",
    "\n",
    "# Print length of pos ngrams with >= 2% document frequency\n",
    "df_pos1 = pd.DataFrame(pos_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"])\n",
    "df_pos1[\"doc_freq_ratio\"] = df_pos1[\"doc_freq\"] / total_example_count\n",
    "df_pos1 = df_pos1[df_pos1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_pos1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# Get all pos ngrams with >= 2% document frequency\n",
    "# df_pos1.to_csv(\"pos_ngrams_features.csv\", index=False)\n",
    "\n",
    "# Print frequencies of depths, branching factors, and widths sorted in descending order by size\n",
    "print(\"Depth frequencies (descending):\")\n",
    "for depth, count in sorted(\n",
    "    dep_tree_depth_counter.items(), key=lambda x: x[0], reverse=True\n",
    "):\n",
    "    print(\n",
    "        f\"Depth {depth}: {count} ({count / sum(dep_tree_depth_counter.values()):.2%})\"\n",
    "    )\n",
    "\"\"\"\n",
    "Depth 25: 1 (0.00%)\n",
    "Depth 23: 1 (0.00%)\n",
    "Depth 22: 1 (0.00%)\n",
    "Depth 21: 1 (0.00%)\n",
    "Depth 19: 1 (0.00%)\n",
    "Depth 18: 7 (0.01%)\n",
    "Depth 17: 12 (0.02%)\n",
    "Depth 16: 22 (0.04%)\n",
    "Depth 15: 35 (0.06%)\n",
    "Depth 14: 61 (0.11%)\n",
    "Depth 13: 141 (0.26%)\n",
    "Depth 12: 198 (0.37%)\n",
    "Depth 11: 388 (0.72%)\n",
    "Depth 10: 777 (1.44%)\n",
    "Depth 9: 1435 (2.66%)\n",
    "Depth 8: 2496 (4.63%)\n",
    "Depth 7: 4220 (7.82%)\n",
    "Depth 6: 6437 (11.93%)\n",
    "Depth 5: 8967 (16.62%)\n",
    "Depth 4: 9694 (17.97%)\n",
    "Depth 3: 8546 (15.84%)\n",
    "Depth 2: 6464 (11.98%)\n",
    "Depth 1: 3907 (7.24%)\n",
    "Depth 0: 128 (0.24%)\n",
    "\n",
    "-> 18 depth levels (0-17+)\n",
    "\"\"\"\n",
    "print(\"Branching factor frequencies (descending):\")\n",
    "for factor, count in sorted(\n",
    "    dep_tree_branching_factor_counter.items(), key=lambda x: x[0], reverse=True\n",
    "):\n",
    "    print(\n",
    "        f\"Branching factor {factor}: {count} ({count / sum(dep_tree_branching_factor_counter.values()):.2%})\"\n",
    "    )\n",
    "\"\"\"\n",
    "Branching factor 38: 1 (0.00%)\n",
    "Branching factor 26: 1 (0.00%)\n",
    "Branching factor 24: 2 (0.00%)\n",
    "Branching factor 20: 1 (0.00%)\n",
    "Branching factor 19: 2 (0.00%)\n",
    "Branching factor 18: 3 (0.00%)\n",
    "Branching factor 17: 5 (0.00%)\n",
    "Branching factor 16: 12 (0.00%)\n",
    "Branching factor 15: 22 (0.01%)\n",
    "Branching factor 14: 54 (0.01%)\n",
    "Branching factor 13: 115 (0.03%)\n",
    "Branching factor 12: 219 (0.05%)\n",
    "Branching factor 11: 545 (0.13%)\n",
    "Branching factor 10: 1134 (0.27%)\n",
    "Branching factor 9: 2218 (0.53%)\n",
    "Branching factor 8: 4308 (1.04%)\n",
    "Branching factor 7: 7768 (1.87%)\n",
    "Branching factor 6: 13773 (3.31%)\n",
    "Branching factor 5: 21412 (5.15%)\n",
    "Branching factor 4: 30381 (7.30%)\n",
    "Branching factor 3: 50672 (12.18%)\n",
    "Branching factor 2: 82596 (19.85%)\n",
    "Branching factor 1: 200918 (48.28%)\n",
    "\n",
    "-> 18 branching factor levels (1-18+)\n",
    "\"\"\"\n",
    "print(\"Width frequencies (descending):\")\n",
    "for width, count in sorted(\n",
    "    dep_tree_width_counter.items(), key=lambda x: x[0], reverse=True\n",
    "):\n",
    "    print(\n",
    "        f\"Width {width}: {count} ({count / sum(dep_tree_width_counter.values()):.2%})\"\n",
    "    )\n",
    "\"\"\"\n",
    "Width 68: 2 (0.00%)\n",
    "Width 66: 2 (0.00%)\n",
    "Width 65: 1 (0.00%)\n",
    "Width 64: 2 (0.00%)\n",
    "Width 63: 3 (0.01%)\n",
    "Width 61: 2 (0.00%)\n",
    "Width 60: 1 (0.00%)\n",
    "Width 59: 3 (0.01%)\n",
    "Width 58: 3 (0.01%)\n",
    "Width 57: 2 (0.00%)\n",
    "Width 56: 7 (0.01%)\n",
    "Width 55: 8 (0.01%)\n",
    "Width 54: 6 (0.01%)\n",
    "Width 53: 7 (0.01%)\n",
    "Width 52: 6 (0.01%)\n",
    "Width 51: 5 (0.01%)\n",
    "Width 50: 14 (0.03%)\n",
    "Width 49: 19 (0.04%)\n",
    "Width 48: 8 (0.01%)\n",
    "Width 47: 17 (0.03%)\n",
    "Width 46: 18 (0.03%)\n",
    "Width 45: 13 (0.02%)\n",
    "Width 44: 35 (0.06%)\n",
    "Width 43: 29 (0.05%)\n",
    "Width 42: 37 (0.07%)\n",
    "Width 41: 44 (0.08%)\n",
    "Width 40: 55 (0.10%)\n",
    "Width 39: 65 (0.12%)\n",
    "Width 38: 67 (0.12%)\n",
    "Width 37: 63 (0.12%)\n",
    "Width 36: 79 (0.15%)\n",
    "Width 35: 76 (0.14%)\n",
    "Width 34: 110 (0.20%)\n",
    "Width 33: 123 (0.23%)\n",
    "Width 32: 133 (0.25%)\n",
    "Width 31: 155 (0.29%)\n",
    "Width 30: 157 (0.29%)\n",
    "Width 29: 221 (0.41%)\n",
    "Width 28: 233 (0.43%)\n",
    "Width 27: 288 (0.53%)\n",
    "Width 26: 306 (0.57%)\n",
    "Width 25: 372 (0.69%)\n",
    "Width 24: 405 (0.75%)\n",
    "Width 23: 549 (1.02%)\n",
    "Width 22: 562 (1.04%)\n",
    "Width 21: 658 (1.22%)\n",
    "Width 20: 855 (1.59%)\n",
    "Width 19: 956 (1.77%)\n",
    "Width 18: 1201 (2.23%)\n",
    "Width 17: 1408 (2.61%)\n",
    "Width 16: 1575 (2.92%)\n",
    "Width 15: 1901 (3.52%)\n",
    "Width 14: 2233 (4.14%)\n",
    "Width 13: 2439 (4.52%)\n",
    "Width 12: 2748 (5.09%)\n",
    "Width 11: 3123 (5.79%)\n",
    "Width 10: 3440 (6.38%)\n",
    "Width 9: 3806 (7.06%)\n",
    "Width 8: 4227 (7.84%)\n",
    "Width 7: 4260 (7.90%)\n",
    "Width 6: 4158 (7.71%)\n",
    "Width 5: 3661 (6.79%)\n",
    "Width 4: 3107 (5.76%)\n",
    "Width 3: 2088 (3.87%)\n",
    "Width 2: 1181 (2.19%)\n",
    "Width 1: 602 (1.12%)\n",
    "\n",
    "-> 56 width levels (1-56+)\n",
    "\"\"\"\n",
    "\n",
    "# Get length of node, relation, complete ngrams with >= 2% document frequency\n",
    "df_node1 = pd.DataFrame(node_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"])\n",
    "df_node1[\"doc_freq_ratio\"] = df_node1[\"doc_freq\"] / total_example_count\n",
    "df_node1 = df_node1[df_node1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_node1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# df_node1.to_csv(\"dep_tree_node_ngrams_features.csv\", index=False)\n",
    "\n",
    "df_relation1 = pd.DataFrame(\n",
    "    relation_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"]\n",
    ")\n",
    "df_relation1[\"doc_freq_ratio\"] = df_relation1[\"doc_freq\"] / total_example_count\n",
    "df_relation1 = df_relation1[df_relation1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_relation1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# df_relation1.to_csv(\"dep_tree_relation_ngrams_features.csv\", index=False)\n",
    "\n",
    "df_complete1 = pd.DataFrame(\n",
    "    complete_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"]\n",
    ")\n",
    "df_complete1[\"doc_freq_ratio\"] = df_complete1[\"doc_freq\"] / total_example_count\n",
    "df_complete1 = df_complete1[df_complete1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_complete1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# df_complete1.to_csv(\"dep_tree_complete_ngrams_features.csv\", index=False)\n",
    "\n",
    "df_noun_phrase_lengths = pd.DataFrame(\n",
    "    np_length_ctr.most_common(), columns=[\"length\", \"count\"]\n",
    ")\n",
    "print(df_noun_phrase_lengths.sort_values(\"length\"))\n",
    "\n",
    "# print(\n",
    "#     f\"Concreteness matches: {concr_matches}, effective word count: {concr_effective_word_count}, ratio: {concr_matches / concr_effective_word_count:.2%}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a50861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preposition matches: 93702, effective word count: 97975, ratio: 95.64%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Preposition matches: {prep_matches}, effective word count: {prep_effective_word_count}, ratio: {prep_matches / prep_effective_word_count:.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved csv's as a list of ngrams and print the counts\n",
    "char_features = pd.read_csv(DATA_DIR / \"features\" / \"char_ngrams_features.csv\")[\n",
    "    \"ngram\"\n",
    "].tolist()\n",
    "print(f\"Loaded {len(char_features)} char ngrams\")\n",
    "pos_features = pd.read_csv(DATA_DIR / \"features\" / \"pos_ngrams_features.csv\")[\n",
    "    \"ngram\"\n",
    "].tolist()\n",
    "print(f\"Loaded {len(pos_features)} pos ngrams\")\n",
    "node_features = pd.read_csv(\n",
    "    DATA_DIR / \"features\" / \"dep_tree_node_ngrams_features.csv\"\n",
    ")[\"ngram\"].tolist()\n",
    "print(f\"Loaded {len(node_features)} node ngrams\")\n",
    "relation_features = pd.read_csv(\n",
    "    DATA_DIR / \"features\" / \"dep_tree_relation_ngrams_features.csv\"\n",
    ")[\"ngram\"].tolist()\n",
    "print(f\"Loaded {len(relation_features)} relation ngrams\")\n",
    "complete_features = pd.read_csv(\n",
    "    DATA_DIR / \"features\" / \"dep_tree_complete_ngrams_features.csv\"\n",
    ")[\"ngram\"].tolist()\n",
    "print(f\"Loaded {len(complete_features)} complete ngrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01765adf",
   "metadata": {},
   "source": [
    "# Big dataset heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b23dc",
   "metadata": {},
   "source": [
    "Get training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import json\n",
    "\n",
    "dataset_to_scores = [\n",
    "    [5],\n",
    "    [4],\n",
    "    [4, 3, 2],\n",
    "    [3, 2],\n",
    "    [4, 3, 2, 1],\n",
    "    [3, 2, 1, 0],\n",
    "    [3, 2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [],\n",
    "]\n",
    "\n",
    "\n",
    "def create_labeling_interface(\n",
    "    output_filename: str,\n",
    "    samples_per_dataset: int = 50,\n",
    "    seed: int = 42,\n",
    "    exclude_samples: dict[int, set[int]] = None,\n",
    "):\n",
    "    \"\"\"Create a Gradio interface for labeling dataset examples.\n",
    "\n",
    "    Args:\n",
    "        output_filename: Name of the file to save progress and results (without extension)\n",
    "        samples_per_dataset: Number of samples to label per dataset\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Filter datasets that have more than one possible score\n",
    "    datasets_to_label = [\n",
    "        (i, ds, scores)\n",
    "        for i, (ds, scores) in enumerate(zip(datasets, dataset_to_scores))\n",
    "        if len(scores) > 1\n",
    "    ]\n",
    "\n",
    "    # Get random samples\n",
    "    samples_to_label = []\n",
    "    for dataset_idx, dataset, scores in datasets_to_label:\n",
    "        if dataset.processed and len(dataset.processed) >= samples_per_dataset:\n",
    "            population = set(range(len(dataset.processed))) - (\n",
    "                exclude_samples.get(dataset_idx, set()) if exclude_samples else set()\n",
    "            )\n",
    "            sampled_indices = random.sample(\n",
    "                sorted(population), min(samples_per_dataset, len(population))\n",
    "            )\n",
    "            for idx in sampled_indices:\n",
    "                samples_to_label.append(\n",
    "                    {\n",
    "                        \"dataset_idx\": dataset_idx,\n",
    "                        \"example_idx\": idx,\n",
    "                        \"text\": dataset.processed[idx].text,\n",
    "                        \"possible_scores\": scores,\n",
    "                        \"score\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Load progress if exists\n",
    "    progress_file = DATA_DIR / \"datasets\" / \"large\" / f\"{output_filename}_progress.json\"\n",
    "    if progress_file.exists():\n",
    "        with open(progress_file, \"r\") as f:\n",
    "            progress_data = json.load(f)\n",
    "            samples_to_label = progress_data[\"samples\"]\n",
    "            # Update possible scores in case they changed\n",
    "            for sample in samples_to_label:\n",
    "                dataset_idx = sample[\"dataset_idx\"]\n",
    "                sample[\"possible_scores\"] = dataset_to_scores[dataset_idx]\n",
    "            current_idx = progress_data.get(\"current_idx\", 0)\n",
    "    else:\n",
    "        current_idx = 0\n",
    "\n",
    "    # Count labeled examples\n",
    "    labeled_count = sum(1 for s in samples_to_label if s[\"score\"] is not None)\n",
    "\n",
    "    def save_progress():\n",
    "        with open(progress_file, \"w\") as f:\n",
    "            json.dump(\n",
    "                {\"samples\": samples_to_label, \"current_idx\": current_idx}, f, indent=2\n",
    "            )\n",
    "\n",
    "    def get_next_unlabeled():\n",
    "        nonlocal current_idx\n",
    "        for i in range(current_idx, len(samples_to_label)):\n",
    "            if samples_to_label[i][\"score\"] is None:\n",
    "                current_idx = i\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def label_example(score):\n",
    "        nonlocal current_idx, labeled_count\n",
    "        if current_idx < len(samples_to_label):\n",
    "            samples_to_label[current_idx][\"score\"] = score\n",
    "            labeled_count += 1\n",
    "            save_progress()\n",
    "\n",
    "            # Move to next unlabeled\n",
    "            next_idx = get_next_unlabeled()\n",
    "            if next_idx is not None:\n",
    "                current_idx = next_idx\n",
    "                sample = samples_to_label[current_idx]\n",
    "                progress_text = f\"Example {current_idx + 1} / {len(samples_to_label)} (Labeled: {labeled_count})\"\n",
    "                return (\n",
    "                    sample[\"text\"],\n",
    "                    gr.update(choices=sample[\"possible_scores\"]),\n",
    "                    progress_text,\n",
    "                )\n",
    "            else:\n",
    "                # All done - save to CSV\n",
    "                df_results = pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"dataset_idx\": s[\"dataset_idx\"],\n",
    "                            \"example_idx\": s[\"example_idx\"],\n",
    "                            \"text\": s[\"text\"],\n",
    "                            \"possible_scores\": s[\"possible_scores\"],\n",
    "                            \"score\": s[\"score\"],\n",
    "                        }\n",
    "                        for s in samples_to_label\n",
    "                        if s[\"score\"] is not None\n",
    "                    ]\n",
    "                )\n",
    "                df_results.to_csv(\n",
    "                    DATA_DIR / \"datasets\" / \"large\" / f\"{output_filename}.csv\",\n",
    "                    index=False,\n",
    "                )\n",
    "                return (\n",
    "                    \"All examples labeled! Results saved.\",\n",
    "                    gr.update(choices=[]),\n",
    "                    f\"Complete: {labeled_count} / {len(samples_to_label)}\",\n",
    "                )\n",
    "\n",
    "        return \"No more examples\", gr.update(choices=[]), \"Complete\"\n",
    "\n",
    "    # Initialize interface\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(f\"# Dataset Labeling Interface - {output_filename}\")\n",
    "\n",
    "        progress = gr.Textbox(\n",
    "            label=\"Progress\",\n",
    "            value=f\"Example {current_idx + 1} / {len(samples_to_label)} (Labeled: {labeled_count})\",\n",
    "            interactive=False,\n",
    "        )\n",
    "\n",
    "        initial_sample = (\n",
    "            samples_to_label[current_idx]\n",
    "            if current_idx < len(samples_to_label)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        text_display = gr.Textbox(\n",
    "            label=\"Text to Label\",\n",
    "            value=initial_sample[\"text\"] if initial_sample else \"\",\n",
    "            lines=10,\n",
    "            interactive=False,\n",
    "        )\n",
    "\n",
    "        score_radio = gr.Radio(\n",
    "            choices=initial_sample[\"possible_scores\"] if initial_sample else [],\n",
    "            label=\"Select Score\",\n",
    "        )\n",
    "\n",
    "        submit_btn = gr.Button(\"Submit and Next\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=label_example,\n",
    "            inputs=[score_radio],\n",
    "            outputs=[text_display, score_radio, progress],\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f70f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = create_labeling_interface(\"heuristic_train_set\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140c2a0",
   "metadata": {},
   "source": [
    "Get validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(DATA_DIR / \"datasets\" / \"large\" / \"heuristic_train_set.csv\")\n",
    "exclude_samples = {}\n",
    "for _, row in train_set.iterrows():\n",
    "    ds_idx = int(row[\"dataset_idx\"])\n",
    "    ex_idx = int(row[\"example_idx\"])\n",
    "    if ds_idx not in exclude_samples:\n",
    "        exclude_samples[ds_idx] = set()\n",
    "    exclude_samples[ds_idx].add(ex_idx)\n",
    "\n",
    "demo = create_labeling_interface(\n",
    "    \"heuristic_validation_set\", samples_per_dataset=15, exclude_samples=exclude_samples\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9123c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b7849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV Accuracy: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Tried:\n",
    "# from mord import LogisticAT, LogisticIT, OrdinalRidge\n",
    "# from sklearn.svm import LinearSVR\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "\n",
    "train_set = pd.read_csv(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"heuristic_train_set_combined.csv\"\n",
    ")\n",
    "cv_scores = []\n",
    "\n",
    "for dataset_idx, group in train_set.groupby(\"dataset_idx\"):\n",
    "    X = np.array([feature_extractor.extract(text) for text in group[\"text\"]])\n",
    "    y = group[\"score\"]\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(X, y, sample_weight=compute_sample_weight(\"balanced\", y))\n",
    "    with open(\n",
    "        DATA_DIR / \"models\" / f\"ordinal_model_dataset_{dataset_idx}.pkl\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # scores = cross_val_score(GaussianNB(), X, y, cv=min(5, len(X)), scoring='accuracy', params={'sample_weight': compute_sample_weight('balanced', y)})\n",
    "    # mean_score = scores.mean()\n",
    "    # std_score = scores.std()\n",
    "    # cv_scores.append(mean_score)\n",
    "    # print(f\"Dataset {dataset_idx}: CV Accuracy = {mean_score:.2%} (+/- {std_score:.2%})\")\n",
    "\n",
    "print(f\"\\nOverall CV Accuracy: {np.mean(cv_scores):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
