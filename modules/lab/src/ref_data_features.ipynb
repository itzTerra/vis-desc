{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899d3523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-12 13:30:43.899483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760268643.934909 1215969 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760268643.944707 1215969 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-12 13:30:43.979588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Iterable\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from numpy.typing import NDArray\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import ZODB\n",
    "import ZODB.FileStorage\n",
    "import transaction\n",
    "from features import FeatureExtractorPipeline, ExtCtx, SentenceToken\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from book_segmenting import TextSegmenter\n",
    "from utils import DATA_DIR\n",
    "\n",
    "feature_extractor = FeatureExtractorPipeline()\n",
    "\n",
    "SEGMENT_CHARS_MIN = 150\n",
    "SEGMENT_CHARS_MAX = 500\n",
    "segmenter = TextSegmenter(chunk_size=(SEGMENT_CHARS_MIN, SEGMENT_CHARS_MAX))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    MIN_TEXT_LENGTH = 60\n",
    "    MAX_TEXT_LENGTH = 500\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src: Iterable[Any],\n",
    "        take: int,\n",
    "        skip: int = 0,\n",
    "        text_getter=None,\n",
    "        deduplicate=False,\n",
    "        segment=False,\n",
    "        check_length=True,\n",
    "    ):\n",
    "        self.src = iter(src)\n",
    "        self.take = take\n",
    "        self.skip = skip\n",
    "        self.contexts: list[ExtCtx] | None = None\n",
    "        self.features: list[NDArray[np.float32]] | None = None\n",
    "        self.text_getter = text_getter\n",
    "        self.deduplicate = deduplicate\n",
    "        self.segment = segment\n",
    "        self.check_length = check_length\n",
    "\n",
    "    def process(\n",
    "        self, deduplicate: bool | None = None, segment: bool | None = None\n",
    "    ) -> list[ExtCtx]:\n",
    "        if deduplicate is None:\n",
    "            deduplicate = self.deduplicate\n",
    "        if segment is None:\n",
    "            segment = self.segment\n",
    "\n",
    "        self.contexts = []\n",
    "        self.features = []\n",
    "        if deduplicate:\n",
    "            seen = set()\n",
    "        taken = 0\n",
    "        to_skip = self.skip\n",
    "\n",
    "        with tqdm(total=self.take, desc=\"Processing texts\", unit=\"text\") as pbar:\n",
    "            while taken < self.take:\n",
    "                try:\n",
    "                    text = next(self.src)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                if to_skip > 0:\n",
    "                    to_skip -= 1\n",
    "                    continue\n",
    "\n",
    "                if self.text_getter is not None:\n",
    "                    text = self.text_getter(text)\n",
    "                if not text or (\n",
    "                    self.check_length and len(text.strip()) < Dataset.MIN_TEXT_LENGTH\n",
    "                ):\n",
    "                    continue\n",
    "                if deduplicate:\n",
    "                    if text in seen:\n",
    "                        continue\n",
    "                    seen.add(text)\n",
    "\n",
    "                text = FeatureExtractorPipeline.preprocess(text)\n",
    "                if self.check_length and len(text.strip()) < Dataset.MIN_TEXT_LENGTH:\n",
    "                    continue\n",
    "\n",
    "                if segment:\n",
    "                    segments = [\n",
    "                        seg\n",
    "                        for seg in segmenter.segment_text(text)\n",
    "                        if seg\n",
    "                        and (\n",
    "                            not self.check_length\n",
    "                            or (seg_len := len(seg.strip())) >= Dataset.MIN_TEXT_LENGTH\n",
    "                            and seg_len <= Dataset.MAX_TEXT_LENGTH\n",
    "                        )\n",
    "                    ]\n",
    "                    if len(segments) == 0:\n",
    "                        continue\n",
    "                    example = segments[len(segments) // 2]\n",
    "                    ctx = feature_extractor.get_ctx(example)\n",
    "                    self.contexts.append(ctx)\n",
    "                    self.features.append(\n",
    "                        feature_extractor.extract(example, preprocess=False, ctx=ctx)\n",
    "                    )\n",
    "                else:\n",
    "                    if self.check_length and len(text) > Dataset.MAX_TEXT_LENGTH:\n",
    "                        continue\n",
    "                    ctx = feature_extractor.get_ctx(text)\n",
    "                    self.contexts.append(ctx)\n",
    "                    self.features.append(\n",
    "                        feature_extractor.extract(text, preprocess=False, ctx=ctx)\n",
    "                    )\n",
    "                taken += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        return self.contexts\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.contexts is None:\n",
    "            raise ValueError(\"Dataset not processed yet. Call process() first.\")\n",
    "        return iter(self.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18499766",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_high_flickr = load_dataset(\n",
    "    \"CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c10991",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_flickr30k = load_dataset(\"embedding-data/flickr30k_captions_quintets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b2a2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_coco = load_dataset(\"sentence-transformers/coco-captions\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffb2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sbu = load_dataset(\"vicenteor/sbu_captions\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"movie_summaries.txt\") as f:\n",
    "    ds_movie_summaries = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d47a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_book_summaries = load_dataset(\"textminr/cmu-book-summaries\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"book_dialogs.txt\") as f:\n",
    "    ds_book_dialogs = [line.strip() for line in f.read().split(\"\\n\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95984a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b8301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_news = load_dataset(\"EdinburghNLP/xsum\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07bd44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hotels = load_dataset(\"argilla/tripadvisor-hotel-reviews\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f04a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_yelp = load_dataset(\"Yelp/yelp_review_full\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1b5d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_arxiv = load_dataset(\n",
    "    \"armanc/scientific_papers\",\n",
    "    \"arxiv\",\n",
    "    split=\"validation\",\n",
    "    trust_remote_code=True,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac74e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMAZON_CATEGORIES = [\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Beauty_and_Personal_Care\",\n",
    "    \"Electronics\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"CDs_and_Vinyl\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Software\",\n",
    "]\n",
    "ds_amazon_reviews = []\n",
    "N_TOTAL = 2000\n",
    "for category in AMAZON_CATEGORIES:\n",
    "    ds = iter(\n",
    "        load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{category}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "            streaming=True,\n",
    "        )\n",
    "    )\n",
    "    for i in range(N_TOTAL // len(AMAZON_CATEGORIES)):\n",
    "        ds_amazon_reviews.append(next(ds))\n",
    "while len(ds_amazon_reviews) < N_TOTAL:\n",
    "    ds_amazon_reviews.append(next(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25eaa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"batch_10k.json\") as f:\n",
    "    books_10k_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d201210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 0/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1500/1500 [04:06<00:00,  6.08text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 1/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:   0%|          | 0/1500 [00:00<?, ?text/s]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 9f8e6406-b986-4152-94d3-d821a5a34bdc)')' thrown while requesting GET https://huggingface.co/datasets/CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption/resolve/75b33ce72533023bf907f8a0bf160099883f1bae/train/train_0000.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Processing texts: 100%|██████████| 1500/1500 [02:21<00:00, 10.56text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 2/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1500/1500 [00:56<00:00, 26.68text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 3/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:17<00:00, 29.10text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 4/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:19<00:00, 25.07text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 5/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:32<00:00, 15.26text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 6/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:34<00:00, 14.51text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 7/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:46<00:00, 10.68text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 8/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 1000/1000 [01:22<00:00, 12.10text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 9/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:34<00:00, 14.38text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 10/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 200/200 [00:15<00:00, 12.71text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 11/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 300/300 [00:27<00:00, 10.91text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 12/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:57<00:00,  8.67text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 13/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 500/500 [00:30<00:00, 16.14text/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    Dataset(ds_high_flickr, take=1500, text_getter=lambda x: x[\"caption_internlm2\"]),\n",
    "    Dataset(\n",
    "        ds_high_flickr, take=1500, text_getter=lambda x: x[\"caption_internlm2_short\"]\n",
    "    ),\n",
    "    Dataset(ds_flickr30k, take=1500, text_getter=lambda x: x[\"set\"][0]),\n",
    "    Dataset(ds_coco, take=500, text_getter=lambda x: x[\"caption1\"]),\n",
    "    Dataset(ds_sbu, take=500, text_getter=lambda x: x[\"caption\"]),\n",
    "    Dataset(ds_movie_summaries, take=500, segment=True),\n",
    "    Dataset(\n",
    "        ds_book_summaries, take=500, text_getter=lambda x: x[\"summary\"], segment=True\n",
    "    ),\n",
    "    Dataset(ds_book_dialogs, take=500),\n",
    "    Dataset(\n",
    "        ds_wiki,\n",
    "        take=1000,\n",
    "        text_getter=lambda x: x[\"text\"].replace(\" @-@ \", \"-\").replace(\" @,@ \", \",\"),\n",
    "        segment=True,\n",
    "    ),\n",
    "    Dataset(ds_news, take=500, text_getter=lambda x: x[\"document\"], segment=True),\n",
    "    Dataset(ds_hotels, take=200, text_getter=lambda x: x[\"text\"]),\n",
    "    Dataset(ds_yelp, take=300, text_getter=lambda x: x[\"text\"]),\n",
    "    Dataset(ds_arxiv, take=500, text_getter=lambda x: x[\"abstract\"], segment=True),\n",
    "    Dataset(ds_amazon_reviews, take=500, text_getter=lambda x: x[\"text\"]),\n",
    "    # Dataset(books_10k_dataset, take=10000, text_getter=lambda x: x[\"text\"], check_length=False),\n",
    "]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f\"--- Done: {i}/{len(datasets)} ---\")\n",
    "    dataset.process(deduplicate=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07597623",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = ZODB.FileStorage.FileStorage(\"../data/db/mydata.fs\")\n",
    "db = ZODB.DB(storage)\n",
    "connection = db.open()\n",
    "root = connection.root\n",
    "\n",
    "# root.datasets = BTrees.OOBTree.BTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZODB Load\n",
    "datasets_features: list[NDArray[np.float32]] = root.datasets[\"ref_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647f3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/ZODB/Connection.py:574: UserWarning: The <class 'BTrees.OOBTree.OOBTree'>\n",
      "object you're saving is large. (1622460330 bytes.)\n",
      "\n",
      "Perhaps you're storing media which should be stored in blobs.\n",
      "\n",
      "Perhaps you're using a non-scalable data structure, such as a\n",
      "PersistentMapping or PersistentList.\n",
      "\n",
      "Perhaps you're storing data in objects that aren't persistent at\n",
      "all. In cases like that, the data is stored in the record of the\n",
      "containing persistent object.\n",
      "\n",
      "In any case, storing records this big is probably a bad idea.\n",
      "\n",
      "If you insist and want to get rid of this warning, use the\n",
      "large_record_size option of the ZODB.DB constructor (or the\n",
      "large-record-size option in a configuration file) to specify a larger\n",
      "size.\n",
      "\n",
      "  warnings.warn(large_object_message % (obj.__class__, len(p)))\n"
     ]
    }
   ],
   "source": [
    "# ZODB Save\n",
    "def save_datasets(datasets: list[Dataset], name: str):\n",
    "    root.datasets[f\"{name}_contexts\"] = [ds.contexts for ds in datasets]\n",
    "    root.datasets[f\"{name}_features\"] = [ds.features for ds in datasets]\n",
    "    transaction.commit()\n",
    "\n",
    "\n",
    "save_datasets(datasets, name=\"ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_char_ngrams(ctx: ExtCtx, ctr: Counter, document_ctr: Counter) -> Counter:\n",
    "    \"\"\"Required ctx: 'text'\n",
    "\n",
    "    Time complexity: O(mn), where m is number of characters and n is n-gram size\n",
    "    \"\"\"\n",
    "    MAX_N = 5\n",
    "\n",
    "    document_set = set()\n",
    "    text = ctx.text.casefold()\n",
    "    for n in range(2, MAX_N + 1):\n",
    "        for i in range(len(text) - n + 1):\n",
    "            ngram = text[i : i + n]\n",
    "            ctr[ngram] += 1\n",
    "            document_set.add(ngram)\n",
    "    document_ctr.update(document_set)\n",
    "\n",
    "\n",
    "def extract_pos_ngrams(ctx: ExtCtx, ctr: Counter, document_ctr: Counter) -> Counter:\n",
    "    \"\"\"Required ctx: 'tokens'\n",
    "\n",
    "    Time complexity: O(mn), where m is number of tokens and n is n-gram size\n",
    "\n",
    "    Uses coarse UD tags:\n",
    "    ADJ: adjective\n",
    "    ADP: adposition\n",
    "    ADV: adverb\n",
    "    AUX: auxiliary\n",
    "    CCONJ: coordinating conjunction\n",
    "    DET: determiner\n",
    "    INTJ: interjection\n",
    "    NOUN: noun\n",
    "    NUM: numeral\n",
    "    PART: particle\n",
    "    PRON: pronoun\n",
    "    PROPN: proper noun\n",
    "    PUNCT: punctuation\n",
    "    SCONJ: subordinating conjunction\n",
    "    SYM: symbol\n",
    "    VERB: verb\n",
    "    X: other\n",
    "    \"\"\"\n",
    "    MAX_N = 4\n",
    "\n",
    "    document_set = set()\n",
    "    tokens = ctx.tokens\n",
    "    for n in range(2, MAX_N + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(token.pos for token in tokens[i : i + n])\n",
    "            ctr[ngram] += 1\n",
    "            document_set.add(ngram)\n",
    "    document_ctr.update(document_set)\n",
    "\n",
    "\n",
    "def extract_dependency_tree_structure(\n",
    "    ctx: ExtCtx, depth_ctr: Counter, branching_factor_ctr: Counter, width_ctr: Counter\n",
    "):\n",
    "    \"\"\"Required ctx: 'sents'\n",
    "\n",
    "    Complexity: O(n), where n is number of tokens in all sentences\n",
    "    \"\"\"\n",
    "\n",
    "    def get_tree_depth(token: SentenceToken):\n",
    "        \"\"\"Compute the longest path from root to any leaf node.\"\"\"\n",
    "        if not list(token.children):  # leaf node\n",
    "            return 0\n",
    "        return 1 + max(get_tree_depth(child) for child in token.children)\n",
    "\n",
    "    def get_branching_factors(token: SentenceToken, factors=None):\n",
    "        \"\"\"Get branching factor for each non-leaf node.\"\"\"\n",
    "        if factors is None:\n",
    "            factors = []\n",
    "\n",
    "        children = list(token.children)\n",
    "        if children:\n",
    "            factors.append(len(children))\n",
    "            for child in children:\n",
    "                get_branching_factors(child, factors)\n",
    "\n",
    "        return factors\n",
    "\n",
    "    def count_leaf_nodes(token: SentenceToken):\n",
    "        \"\"\"Count the number of leaf nodes in the tree.\"\"\"\n",
    "        children = list(token.children)\n",
    "        if not children:\n",
    "            return 1\n",
    "        return sum(count_leaf_nodes(child) for child in children)\n",
    "\n",
    "    for sent in ctx.sents:\n",
    "        root = sent.root\n",
    "\n",
    "        depth = get_tree_depth(root)\n",
    "        depth_ctr[depth] += 1\n",
    "\n",
    "        factors = get_branching_factors(root)\n",
    "        branching_factor_ctr.update(factors)\n",
    "\n",
    "        width = count_leaf_nodes(root)\n",
    "        width_ctr[width] += 1\n",
    "\n",
    "\n",
    "def extract_dependency_tree_relations(\n",
    "    ctx: ExtCtx,\n",
    "    node_ngrams: Counter,\n",
    "    relation_ngrams: Counter,\n",
    "    complete_ngrams: Counter,\n",
    "    node_doc_freq: Counter,\n",
    "    relation_doc_freq: Counter,\n",
    "    complete_doc_freq: Counter,\n",
    "):\n",
    "    \"\"\"Required ctx: 'sents'\n",
    "\n",
    "    Returns three counters:\n",
    "    1. Node n-grams (2-4-grams) - ascending path of node labels (POS tags)\n",
    "    2. Relation n-grams (1-4-grams) - ascending path of edge labels (dependency relations)\n",
    "    3. Complete n-grams (2-4-grams) - path with both node and edge labels\n",
    "    \"\"\"\n",
    "    node_ngrams_set = set()\n",
    "    relation_ngrams_set = set()\n",
    "    complete_ngrams_set = set()\n",
    "\n",
    "    def get_ascending_paths(\n",
    "        token: SentenceToken,\n",
    "        current_path_nodes=None,\n",
    "        current_path_rels=None,\n",
    "        visited=None,\n",
    "    ):\n",
    "        \"\"\"Get all ascending paths starting from this token\"\"\"\n",
    "        if current_path_nodes is None:\n",
    "            current_path_nodes = []\n",
    "        if current_path_rels is None:\n",
    "            current_path_rels = []\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "\n",
    "        if token.text in visited:  # Avoid cycles\n",
    "            return\n",
    "\n",
    "        visited.add(token.text)\n",
    "        current_path_nodes.append(token.pos)\n",
    "\n",
    "        # Process current path for node n-grams (2-4)\n",
    "        path_len = len(current_path_nodes)\n",
    "        for n in range(2, min(5, path_len + 1)):\n",
    "            if path_len >= n:\n",
    "                ngram = tuple(current_path_nodes[-n:])\n",
    "                node_ngrams[ngram] += 1\n",
    "                node_ngrams_set.add(ngram)\n",
    "\n",
    "        # Process relation n-grams (1-4)\n",
    "        if current_path_rels:\n",
    "            for n in range(1, min(5, len(current_path_rels) + 1)):\n",
    "                if len(current_path_rels) >= n:\n",
    "                    ngram = tuple(current_path_rels[-n:])\n",
    "                    relation_ngrams[ngram] += 1\n",
    "                    relation_ngrams_set.add(ngram)\n",
    "\n",
    "        # Process complete n-grams (2-4) - alternating node-rel-node\n",
    "        if len(current_path_nodes) >= 2 and len(current_path_rels) >= 1:\n",
    "            for n in range(2, min(5, len(current_path_nodes) + 1)):\n",
    "                if len(current_path_nodes) >= n and len(current_path_rels) >= n - 1:\n",
    "                    complete_path = []\n",
    "                    for i in range(n):\n",
    "                        complete_path.append(current_path_nodes[-(n - i)])\n",
    "                        if i < n - 1 and len(current_path_rels) > (n - 2 - i):\n",
    "                            complete_path.append(current_path_rels[-(n - 1 - i)])\n",
    "                    complete_ngrams[tuple(complete_path)] += 1\n",
    "                    complete_ngrams_set.add(tuple(complete_path))\n",
    "\n",
    "        for child in token.children:\n",
    "            new_path_rels = current_path_rels + [child.dep]\n",
    "            get_ascending_paths(\n",
    "                child, current_path_nodes[:], new_path_rels[:], visited.copy()\n",
    "            )\n",
    "\n",
    "        visited.remove(token.text)\n",
    "\n",
    "    for sent in ctx.sents:\n",
    "        get_ascending_paths(sent.root)\n",
    "\n",
    "    node_doc_freq.update(node_ngrams_set)\n",
    "    relation_doc_freq.update(relation_ngrams_set)\n",
    "    complete_doc_freq.update(complete_ngrams_set)\n",
    "\n",
    "    return node_ngrams, relation_ngrams, complete_ngrams\n",
    "\n",
    "\n",
    "def extract_noun_phrase_lengths(ctx: ExtCtx, np_length_ctr: Counter):\n",
    "    \"\"\"Required ctx: 'noun_chunks'\n",
    "\n",
    "    Time complexity: O(m), where m is number of noun phrases\n",
    "    \"\"\"\n",
    "    for chunk in ctx.noun_chunks:\n",
    "        np_length_ctr[chunk.length] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae94a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter = Counter()\n",
    "char_doc_freq = Counter()\n",
    "pos_counter = Counter()\n",
    "pos_doc_freq = Counter()\n",
    "dep_tree_depth_counter = Counter()\n",
    "dep_tree_branching_factor_counter = Counter()\n",
    "dep_tree_width_counter = Counter()\n",
    "node_ngrams = Counter()  # 2-4-grams of POS tags\n",
    "node_doc_freq = Counter()\n",
    "relation_ngrams = Counter()  # 1-4-grams of dependency relations\n",
    "relation_doc_freq = Counter()\n",
    "complete_ngrams = Counter()  # 2-4-grams of alternating POS-rel-POS patterns\n",
    "complete_doc_freq = Counter()\n",
    "np_length_ctr = Counter()\n",
    "concr_matches = 0\n",
    "concr_effective_word_count = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    for ctx in dataset:\n",
    "        extract_char_ngrams(ctx, char_counter, char_doc_freq)\n",
    "        extract_pos_ngrams(ctx, pos_counter, pos_doc_freq)\n",
    "        extract_dependency_tree_structure(\n",
    "            ctx,\n",
    "            dep_tree_depth_counter,\n",
    "            dep_tree_branching_factor_counter,\n",
    "            dep_tree_width_counter,\n",
    "        )\n",
    "        extract_dependency_tree_relations(\n",
    "            ctx,\n",
    "            node_ngrams,\n",
    "            relation_ngrams,\n",
    "            complete_ngrams,\n",
    "            node_doc_freq,\n",
    "            relation_doc_freq,\n",
    "            complete_doc_freq,\n",
    "        )\n",
    "        extract_noun_phrase_lengths(ctx, np_length_ctr)\n",
    "        _, _, match_count, effective_word_count = (\n",
    "            feature_extractor.extract_word_concreteness(ctx)\n",
    "        )\n",
    "        concr_matches += match_count\n",
    "        concr_effective_word_count += effective_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab585a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_example_count = sum(ds.take for ds in datasets)\n",
    "\n",
    "# Get top 1000 ngrams closest to 50% document frequency in each dataset\n",
    "df1 = pd.DataFrame(char_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"])\n",
    "df1[\"doc_freq_ratio\"] = df1[\"doc_freq\"] / total_example_count\n",
    "df1[\"doc_freq_diff\"] = np.abs(df1[\"doc_freq_ratio\"] - 0.5)\n",
    "df1 = df1.sort_values(\"doc_freq_diff\").reset_index(drop=True)\n",
    "df1 = df1.head(1000)\n",
    "# df1.to_csv(\"char_ngrams_features.csv\", index=False)\n",
    "\n",
    "# Print length of pos ngrams with >= 2% document frequency\n",
    "df_pos1 = pd.DataFrame(pos_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"])\n",
    "df_pos1[\"doc_freq_ratio\"] = df_pos1[\"doc_freq\"] / total_example_count\n",
    "df_pos1 = df_pos1[df_pos1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_pos1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# Get all pos ngrams with >= 2% document frequency\n",
    "# df_pos1.to_csv(\"pos_ngrams_features.csv\", index=False)\n",
    "\n",
    "# Print frequencies of depths, branching factors, and widths sorted in descending order by size\n",
    "print(\"Depth frequencies (descending):\")\n",
    "for depth, count in sorted(\n",
    "    dep_tree_depth_counter.items(), key=lambda x: x[0], reverse=True\n",
    "):\n",
    "    print(\n",
    "        f\"Depth {depth}: {count} ({count / sum(dep_tree_depth_counter.values()):.2%})\"\n",
    "    )\n",
    "\"\"\"\n",
    "Depth 25: 1 (0.00%)\n",
    "Depth 23: 1 (0.00%)\n",
    "Depth 22: 1 (0.00%)\n",
    "Depth 21: 1 (0.00%)\n",
    "Depth 19: 1 (0.00%)\n",
    "Depth 18: 7 (0.01%)\n",
    "Depth 17: 12 (0.02%)\n",
    "Depth 16: 22 (0.04%)\n",
    "Depth 15: 35 (0.06%)\n",
    "Depth 14: 61 (0.11%)\n",
    "Depth 13: 141 (0.26%)\n",
    "Depth 12: 198 (0.37%)\n",
    "Depth 11: 388 (0.72%)\n",
    "Depth 10: 777 (1.44%)\n",
    "Depth 9: 1435 (2.66%)\n",
    "Depth 8: 2496 (4.63%)\n",
    "Depth 7: 4220 (7.82%)\n",
    "Depth 6: 6437 (11.93%)\n",
    "Depth 5: 8967 (16.62%)\n",
    "Depth 4: 9694 (17.97%)\n",
    "Depth 3: 8546 (15.84%)\n",
    "Depth 2: 6464 (11.98%)\n",
    "Depth 1: 3907 (7.24%)\n",
    "Depth 0: 128 (0.24%)\n",
    "\n",
    "-> 18 depth levels (0-17+)\n",
    "\"\"\"\n",
    "print(\"Branching factor frequencies (descending):\")\n",
    "for factor, count in sorted(\n",
    "    dep_tree_branching_factor_counter.items(), key=lambda x: x[0], reverse=True\n",
    "):\n",
    "    print(\n",
    "        f\"Branching factor {factor}: {count} ({count / sum(dep_tree_branching_factor_counter.values()):.2%})\"\n",
    "    )\n",
    "\"\"\"\n",
    "Branching factor 38: 1 (0.00%)\n",
    "Branching factor 26: 1 (0.00%)\n",
    "Branching factor 24: 2 (0.00%)\n",
    "Branching factor 20: 1 (0.00%)\n",
    "Branching factor 19: 2 (0.00%)\n",
    "Branching factor 18: 3 (0.00%)\n",
    "Branching factor 17: 5 (0.00%)\n",
    "Branching factor 16: 12 (0.00%)\n",
    "Branching factor 15: 22 (0.01%)\n",
    "Branching factor 14: 54 (0.01%)\n",
    "Branching factor 13: 115 (0.03%)\n",
    "Branching factor 12: 219 (0.05%)\n",
    "Branching factor 11: 545 (0.13%)\n",
    "Branching factor 10: 1134 (0.27%)\n",
    "Branching factor 9: 2218 (0.53%)\n",
    "Branching factor 8: 4308 (1.04%)\n",
    "Branching factor 7: 7768 (1.87%)\n",
    "Branching factor 6: 13773 (3.31%)\n",
    "Branching factor 5: 21412 (5.15%)\n",
    "Branching factor 4: 30381 (7.30%)\n",
    "Branching factor 3: 50672 (12.18%)\n",
    "Branching factor 2: 82596 (19.85%)\n",
    "Branching factor 1: 200918 (48.28%)\n",
    "\n",
    "-> 18 branching factor levels (1-18+)\n",
    "\"\"\"\n",
    "print(\"Width frequencies (descending):\")\n",
    "for width, count in sorted(\n",
    "    dep_tree_width_counter.items(), key=lambda x: x[0], reverse=True\n",
    "):\n",
    "    print(\n",
    "        f\"Width {width}: {count} ({count / sum(dep_tree_width_counter.values()):.2%})\"\n",
    "    )\n",
    "\"\"\"\n",
    "Width 68: 2 (0.00%)\n",
    "Width 66: 2 (0.00%)\n",
    "Width 65: 1 (0.00%)\n",
    "Width 64: 2 (0.00%)\n",
    "Width 63: 3 (0.01%)\n",
    "Width 61: 2 (0.00%)\n",
    "Width 60: 1 (0.00%)\n",
    "Width 59: 3 (0.01%)\n",
    "Width 58: 3 (0.01%)\n",
    "Width 57: 2 (0.00%)\n",
    "Width 56: 7 (0.01%)\n",
    "Width 55: 8 (0.01%)\n",
    "Width 54: 6 (0.01%)\n",
    "Width 53: 7 (0.01%)\n",
    "Width 52: 6 (0.01%)\n",
    "Width 51: 5 (0.01%)\n",
    "Width 50: 14 (0.03%)\n",
    "Width 49: 19 (0.04%)\n",
    "Width 48: 8 (0.01%)\n",
    "Width 47: 17 (0.03%)\n",
    "Width 46: 18 (0.03%)\n",
    "Width 45: 13 (0.02%)\n",
    "Width 44: 35 (0.06%)\n",
    "Width 43: 29 (0.05%)\n",
    "Width 42: 37 (0.07%)\n",
    "Width 41: 44 (0.08%)\n",
    "Width 40: 55 (0.10%)\n",
    "Width 39: 65 (0.12%)\n",
    "Width 38: 67 (0.12%)\n",
    "Width 37: 63 (0.12%)\n",
    "Width 36: 79 (0.15%)\n",
    "Width 35: 76 (0.14%)\n",
    "Width 34: 110 (0.20%)\n",
    "Width 33: 123 (0.23%)\n",
    "Width 32: 133 (0.25%)\n",
    "Width 31: 155 (0.29%)\n",
    "Width 30: 157 (0.29%)\n",
    "Width 29: 221 (0.41%)\n",
    "Width 28: 233 (0.43%)\n",
    "Width 27: 288 (0.53%)\n",
    "Width 26: 306 (0.57%)\n",
    "Width 25: 372 (0.69%)\n",
    "Width 24: 405 (0.75%)\n",
    "Width 23: 549 (1.02%)\n",
    "Width 22: 562 (1.04%)\n",
    "Width 21: 658 (1.22%)\n",
    "Width 20: 855 (1.59%)\n",
    "Width 19: 956 (1.77%)\n",
    "Width 18: 1201 (2.23%)\n",
    "Width 17: 1408 (2.61%)\n",
    "Width 16: 1575 (2.92%)\n",
    "Width 15: 1901 (3.52%)\n",
    "Width 14: 2233 (4.14%)\n",
    "Width 13: 2439 (4.52%)\n",
    "Width 12: 2748 (5.09%)\n",
    "Width 11: 3123 (5.79%)\n",
    "Width 10: 3440 (6.38%)\n",
    "Width 9: 3806 (7.06%)\n",
    "Width 8: 4227 (7.84%)\n",
    "Width 7: 4260 (7.90%)\n",
    "Width 6: 4158 (7.71%)\n",
    "Width 5: 3661 (6.79%)\n",
    "Width 4: 3107 (5.76%)\n",
    "Width 3: 2088 (3.87%)\n",
    "Width 2: 1181 (2.19%)\n",
    "Width 1: 602 (1.12%)\n",
    "\n",
    "-> 56 width levels (1-56+)\n",
    "\"\"\"\n",
    "\n",
    "# Get length of node, relation, complete ngrams with >= 2% document frequency\n",
    "df_node1 = pd.DataFrame(node_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"])\n",
    "df_node1[\"doc_freq_ratio\"] = df_node1[\"doc_freq\"] / total_example_count\n",
    "df_node1 = df_node1[df_node1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_node1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# df_node1.to_csv(\"dep_tree_node_ngrams_features.csv\", index=False)\n",
    "\n",
    "df_relation1 = pd.DataFrame(\n",
    "    relation_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"]\n",
    ")\n",
    "df_relation1[\"doc_freq_ratio\"] = df_relation1[\"doc_freq\"] / total_example_count\n",
    "df_relation1 = df_relation1[df_relation1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_relation1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# df_relation1.to_csv(\"dep_tree_relation_ngrams_features.csv\", index=False)\n",
    "\n",
    "df_complete1 = pd.DataFrame(\n",
    "    complete_doc_freq.most_common(), columns=[\"ngram\", \"doc_freq\"]\n",
    ")\n",
    "df_complete1[\"doc_freq_ratio\"] = df_complete1[\"doc_freq\"] / total_example_count\n",
    "df_complete1 = df_complete1[df_complete1[\"doc_freq_ratio\"] >= 0.02]\n",
    "print(df_complete1[\"ngram\"].str.len().value_counts().sort_index())\n",
    "# df_complete1.to_csv(\"dep_tree_complete_ngrams_features.csv\", index=False)\n",
    "\n",
    "df_noun_phrase_lengths = pd.DataFrame(\n",
    "    np_length_ctr.most_common(), columns=[\"length\", \"count\"]\n",
    ")\n",
    "print(df_noun_phrase_lengths.sort_values(\"length\"))\n",
    "\n",
    "print(\n",
    "    f\"Concreteness matches: {concr_matches}, effective word count: {concr_effective_word_count}, ratio: {concr_matches / concr_effective_word_count:.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved csv's as a list of ngrams and print the counts\n",
    "char_features = pd.read_csv(DATA_DIR / \"features\" / \"char_ngrams_features.csv\")[\n",
    "    \"ngram\"\n",
    "].tolist()\n",
    "print(f\"Loaded {len(char_features)} char ngrams\")\n",
    "pos_features = pd.read_csv(DATA_DIR / \"features\" / \"pos_ngrams_features.csv\")[\n",
    "    \"ngram\"\n",
    "].tolist()\n",
    "print(f\"Loaded {len(pos_features)} pos ngrams\")\n",
    "node_features = pd.read_csv(\n",
    "    DATA_DIR / \"features\" / \"dep_tree_node_ngrams_features.csv\"\n",
    ")[\"ngram\"].tolist()\n",
    "print(f\"Loaded {len(node_features)} node ngrams\")\n",
    "relation_features = pd.read_csv(\n",
    "    DATA_DIR / \"features\" / \"dep_tree_relation_ngrams_features.csv\"\n",
    ")[\"ngram\"].tolist()\n",
    "print(f\"Loaded {len(relation_features)} relation ngrams\")\n",
    "complete_features = pd.read_csv(\n",
    "    DATA_DIR / \"features\" / \"dep_tree_complete_ngrams_features.csv\"\n",
    ")[\"ngram\"].tolist()\n",
    "print(f\"Loaded {len(complete_features)} complete ngrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01765adf",
   "metadata": {},
   "source": [
    "# Big dataset heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b23dc",
   "metadata": {},
   "source": [
    "Get training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import json\n",
    "\n",
    "dataset_to_scores = [\n",
    "    [5],\n",
    "    [4],\n",
    "    [4, 3, 2],\n",
    "    [3, 2],\n",
    "    [4, 3, 2, 1],\n",
    "    [3, 2, 1, 0],\n",
    "    [3, 2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [2, 1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [],\n",
    "]\n",
    "\n",
    "\n",
    "def create_labeling_interface(\n",
    "    output_filename: str,\n",
    "    samples_per_dataset: int = 50,\n",
    "    seed: int = 42,\n",
    "    exclude_samples: dict[int, set[int]] = None,\n",
    "):\n",
    "    \"\"\"Create a Gradio interface for labeling dataset examples.\n",
    "\n",
    "    Args:\n",
    "        output_filename: Name of the file to save progress and results (without extension)\n",
    "        samples_per_dataset: Number of samples to label per dataset\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Filter datasets that have more than one possible score\n",
    "    datasets_to_label = [\n",
    "        (i, ds, scores)\n",
    "        for i, (ds, scores) in enumerate(zip(datasets, dataset_to_scores))\n",
    "        if len(scores) > 1\n",
    "    ]\n",
    "\n",
    "    # Get random samples\n",
    "    samples_to_label = []\n",
    "    for dataset_idx, dataset, scores in datasets_to_label:\n",
    "        if dataset.processed and len(dataset.processed) >= samples_per_dataset:\n",
    "            population = set(range(len(dataset.processed))) - (\n",
    "                exclude_samples.get(dataset_idx, set()) if exclude_samples else set()\n",
    "            )\n",
    "            sampled_indices = random.sample(\n",
    "                sorted(population), min(samples_per_dataset, len(population))\n",
    "            )\n",
    "            for idx in sampled_indices:\n",
    "                samples_to_label.append(\n",
    "                    {\n",
    "                        \"dataset_idx\": dataset_idx,\n",
    "                        \"example_idx\": idx,\n",
    "                        \"text\": dataset.processed[idx].text,\n",
    "                        \"possible_scores\": scores,\n",
    "                        \"score\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Load progress if exists\n",
    "    progress_file = DATA_DIR / \"datasets\" / \"large\" / f\"{output_filename}_progress.json\"\n",
    "    if progress_file.exists():\n",
    "        with open(progress_file, \"r\") as f:\n",
    "            progress_data = json.load(f)\n",
    "            samples_to_label = progress_data[\"samples\"]\n",
    "            # Update possible scores in case they changed\n",
    "            for sample in samples_to_label:\n",
    "                dataset_idx = sample[\"dataset_idx\"]\n",
    "                sample[\"possible_scores\"] = dataset_to_scores[dataset_idx]\n",
    "            current_idx = progress_data.get(\"current_idx\", 0)\n",
    "    else:\n",
    "        current_idx = 0\n",
    "\n",
    "    # Count labeled examples\n",
    "    labeled_count = sum(1 for s in samples_to_label if s[\"score\"] is not None)\n",
    "\n",
    "    def save_progress():\n",
    "        with open(progress_file, \"w\") as f:\n",
    "            json.dump(\n",
    "                {\"samples\": samples_to_label, \"current_idx\": current_idx}, f, indent=2\n",
    "            )\n",
    "\n",
    "    def get_next_unlabeled():\n",
    "        nonlocal current_idx\n",
    "        for i in range(current_idx, len(samples_to_label)):\n",
    "            if samples_to_label[i][\"score\"] is None:\n",
    "                current_idx = i\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def label_example(score):\n",
    "        nonlocal current_idx, labeled_count\n",
    "        if current_idx < len(samples_to_label):\n",
    "            samples_to_label[current_idx][\"score\"] = score\n",
    "            labeled_count += 1\n",
    "            save_progress()\n",
    "\n",
    "            # Move to next unlabeled\n",
    "            next_idx = get_next_unlabeled()\n",
    "            if next_idx is not None:\n",
    "                current_idx = next_idx\n",
    "                sample = samples_to_label[current_idx]\n",
    "                progress_text = f\"Example {current_idx + 1} / {len(samples_to_label)} (Labeled: {labeled_count})\"\n",
    "                return (\n",
    "                    sample[\"text\"],\n",
    "                    gr.update(choices=sample[\"possible_scores\"]),\n",
    "                    progress_text,\n",
    "                )\n",
    "            else:\n",
    "                # All done - save to CSV\n",
    "                df_results = pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"dataset_idx\": s[\"dataset_idx\"],\n",
    "                            \"example_idx\": s[\"example_idx\"],\n",
    "                            \"text\": s[\"text\"],\n",
    "                            \"possible_scores\": s[\"possible_scores\"],\n",
    "                            \"score\": s[\"score\"],\n",
    "                        }\n",
    "                        for s in samples_to_label\n",
    "                        if s[\"score\"] is not None\n",
    "                    ]\n",
    "                )\n",
    "                df_results.to_csv(\n",
    "                    DATA_DIR / \"datasets\" / \"large\" / f\"{output_filename}.csv\",\n",
    "                    index=False,\n",
    "                )\n",
    "                return (\n",
    "                    \"All examples labeled! Results saved.\",\n",
    "                    gr.update(choices=[]),\n",
    "                    f\"Complete: {labeled_count} / {len(samples_to_label)}\",\n",
    "                )\n",
    "\n",
    "        return \"No more examples\", gr.update(choices=[]), \"Complete\"\n",
    "\n",
    "    # Initialize interface\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(f\"# Dataset Labeling Interface - {output_filename}\")\n",
    "\n",
    "        progress = gr.Textbox(\n",
    "            label=\"Progress\",\n",
    "            value=f\"Example {current_idx + 1} / {len(samples_to_label)} (Labeled: {labeled_count})\",\n",
    "            interactive=False,\n",
    "        )\n",
    "\n",
    "        initial_sample = (\n",
    "            samples_to_label[current_idx]\n",
    "            if current_idx < len(samples_to_label)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        text_display = gr.Textbox(\n",
    "            label=\"Text to Label\",\n",
    "            value=initial_sample[\"text\"] if initial_sample else \"\",\n",
    "            lines=10,\n",
    "            interactive=False,\n",
    "        )\n",
    "\n",
    "        score_radio = gr.Radio(\n",
    "            choices=initial_sample[\"possible_scores\"] if initial_sample else [],\n",
    "            label=\"Select Score\",\n",
    "        )\n",
    "\n",
    "        submit_btn = gr.Button(\"Submit and Next\")\n",
    "\n",
    "        submit_btn.click(\n",
    "            fn=label_example,\n",
    "            inputs=[score_radio],\n",
    "            outputs=[text_display, score_radio, progress],\n",
    "        )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f70f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = create_labeling_interface(\"heuristic_train_set\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140c2a0",
   "metadata": {},
   "source": [
    "Get validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(DATA_DIR / \"datasets\" / \"large\" / \"heuristic_train_set.csv\")\n",
    "exclude_samples = {}\n",
    "for _, row in train_set.iterrows():\n",
    "    ds_idx = int(row[\"dataset_idx\"])\n",
    "    ex_idx = int(row[\"example_idx\"])\n",
    "    if ds_idx not in exclude_samples:\n",
    "        exclude_samples[ds_idx] = set()\n",
    "    exclude_samples[ds_idx].add(ex_idx)\n",
    "\n",
    "demo = create_labeling_interface(\n",
    "    \"heuristic_validation_set\", samples_per_dataset=15, exclude_samples=exclude_samples\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9123c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV Accuracy: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Tried:\n",
    "# from mord import LogisticAT, LogisticIT, OrdinalRidge\n",
    "# from sklearn.svm import LinearSVR\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "\n",
    "train_set = pd.read_csv(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"heuristic_train_set_combined.csv\"\n",
    ")\n",
    "cv_scores = []\n",
    "\n",
    "# Train ordinal regression model for each dataset\n",
    "for dataset_idx, group in train_set.groupby(\"dataset_idx\"):\n",
    "    X = np.array([feature_extractor.extract(text) for text in group[\"text\"]])\n",
    "    y = group[\"score\"]\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(X, y, sample_weight=compute_sample_weight(\"balanced\", y))\n",
    "    with open(\n",
    "        DATA_DIR / \"models\" / f\"ordinal_model_dataset_{dataset_idx}.pkl\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # scores = cross_val_score(GaussianNB(), X, y, cv=min(5, len(X)), scoring='accuracy', params={'sample_weight': compute_sample_weight('balanced', y)})\n",
    "    # mean_score = scores.mean()\n",
    "    # std_score = scores.std()\n",
    "    # cv_scores.append(mean_score)\n",
    "    # print(f\"Dataset {dataset_idx}: CV Accuracy = {mean_score:.2%} (+/- {std_score:.2%})\")\n",
    "\n",
    "print(f\"\\nOverall CV Accuracy: {np.mean(cv_scores):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_train_set = root.datasets[\"large.v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f1aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled samples: 16109\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 8.89%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 2: 8.89%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 3: 8.89%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 8.89%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 8.89%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 6: 9.44%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 7: 9.44%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 9.44%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 9: 9.44%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 10: 9.44%\n",
      "\n",
      "EM training complete for dataset 2. Final CV Accuracy: 9.44%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 11.11%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 2: 11.11%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 3: 11.11%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 11.11%\n",
      "Convergence: predictions stabilized.\n",
      "\n",
      "EM training complete for dataset 3. Final CV Accuracy: 11.11%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 15.56%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9987\n",
      "Validation Accuracy after iteration 2: 16.67%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 3: 17.78%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 4: 17.78%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 17.78%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 6: 17.78%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 7: 17.78%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 17.78%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 9: 17.78%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 10: 17.78%\n",
      "\n",
      "EM training complete for dataset 4. Final CV Accuracy: 17.78%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 22.22%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9992\n",
      "Validation Accuracy after iteration 2: 21.67%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 3: 22.22%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 4: 22.78%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 5: 22.78%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 6: 21.11%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 7: 21.67%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 22.22%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 9: 22.78%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 10: 23.33%\n",
      "\n",
      "EM training complete for dataset 5. Final CV Accuracy: 23.33%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 22.78%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9987\n",
      "Validation Accuracy after iteration 2: 29.44%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9987\n",
      "Validation Accuracy after iteration 3: 28.89%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 0.9995\n",
      "Validation Accuracy after iteration 4: 28.33%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 5: 28.33%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 6: 27.78%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 7: 27.22%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 8: 27.22%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 9: 27.22%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 10: 27.22%\n",
      "\n",
      "EM training complete for dataset 6. Final CV Accuracy: 27.22%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 40.00%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 2: 40.00%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 3: 40.00%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 40.00%\n",
      "Convergence: predictions stabilized.\n",
      "\n",
      "EM training complete for dataset 7. Final CV Accuracy: 40.00%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 32.78%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9993\n",
      "Validation Accuracy after iteration 2: 35.56%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9996\n",
      "Validation Accuracy after iteration 3: 35.56%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 4: 35.56%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 5: 36.11%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 0.9996\n",
      "Validation Accuracy after iteration 6: 35.56%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 7: 36.11%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 8: 36.11%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 9: 36.11%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 0.9998\n",
      "Validation Accuracy after iteration 10: 35.00%\n",
      "\n",
      "EM training complete for dataset 8. Final CV Accuracy: 35.00%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 40.56%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 2: 40.00%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 3: 40.00%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 40.00%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 40.00%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 6: 40.00%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 7: 40.00%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 40.00%\n",
      "Convergence: predictions stabilized.\n",
      "\n",
      "EM training complete for dataset 9. Final CV Accuracy: 40.00%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 45.00%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 2: 43.89%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 3: 43.33%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 4: 43.89%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 43.89%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 6: 43.89%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 7: 43.89%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 43.89%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 9: 43.89%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 10: 43.89%\n",
      "\n",
      "EM training complete for dataset 10. Final CV Accuracy: 43.89%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 23.33%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9989\n",
      "Validation Accuracy after iteration 2: 27.78%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 3: 26.67%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 27.22%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 27.22%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 0.9999\n",
      "Validation Accuracy after iteration 6: 27.22%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 7: 26.67%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 26.11%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 9: 26.11%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 10: 26.11%\n",
      "\n",
      "EM training complete for dataset 11. Final CV Accuracy: 26.11%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 40.00%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 2: 40.00%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 3: 40.00%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 40.00%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 40.00%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 6: 40.00%\n",
      "Convergence: predictions stabilized.\n",
      "\n",
      "EM training complete for dataset 12. Final CV Accuracy: 40.00%\n",
      "\n",
      "Labeled samples: 50\n",
      "--- EM Iteration 1/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 1: 29.44%\n",
      "--- EM Iteration 2/10 ---\n",
      "Average confidence on unlabeled data: 0.9997\n",
      "Validation Accuracy after iteration 2: 28.89%\n",
      "--- EM Iteration 3/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 3: 28.89%\n",
      "--- EM Iteration 4/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 4: 28.89%\n",
      "--- EM Iteration 5/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 5: 28.89%\n",
      "--- EM Iteration 6/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 6: 28.89%\n",
      "--- EM Iteration 7/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 7: 28.89%\n",
      "--- EM Iteration 8/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 8: 28.89%\n",
      "--- EM Iteration 9/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 9: 28.89%\n",
      "--- EM Iteration 10/10 ---\n",
      "Average confidence on unlabeled data: 1.0000\n",
      "Validation Accuracy after iteration 10: 28.89%\n",
      "\n",
      "EM training complete for dataset 13. Final CV Accuracy: 28.89%\n",
      "\n",
      "\n",
      "Overall CV Accuracy across datasets: 28.56%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Suppress warnings from sklearn about features with zero variance\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "train_set = pd.read_csv(DATA_DIR / \"datasets\" / \"large\" / \"heuristic_train_set.csv\")\n",
    "validation_set = pd.read_csv(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"heuristic_validation_set.csv\"\n",
    ")\n",
    "X_val = np.array([feature_extractor.extract(text) for text in validation_set[\"text\"]])\n",
    "y_val = validation_set[\"score\"].values\n",
    "cv_scores = []\n",
    "\n",
    "# Unlabeled set (U) - all data not in the training or validation sets\n",
    "examples_indices = set(train_set[\"example_idx\"].tolist())\n",
    "X_unlabeled = np.array(\n",
    "    [\n",
    "        ex\n",
    "        for ei, ex in enumerate(datasets[dataset_idx].features)\n",
    "        if ei not in examples_indices\n",
    "    ]\n",
    ")\n",
    "X_unlabeled = np.concatenate([X_unlabeled, large_train_set[dataset_idx]])\n",
    "print(f\"Unlabeled samples: {len(X_unlabeled)}\")\n",
    "\n",
    "for dataset_idx, group in train_set.groupby(\"dataset_idx\"):\n",
    "    # Labeled set (L)\n",
    "    X_labeled = np.array([feature_extractor.extract(text) for text in group[\"text\"]])\n",
    "    y_labeled = group[\"score\"].values\n",
    "    print(f\"Labeled samples: {len(X_labeled)}\")\n",
    "\n",
    "    # --- 2. EM Algorithm Implementation ---\n",
    "    MAX_ITER = 10\n",
    "    model = None\n",
    "    all_classes = np.unique(y_labeled)\n",
    "    prev_predictions = None\n",
    "    base_weights = compute_sample_weight(\"balanced\", y_labeled)\n",
    "\n",
    "    for i in range(MAX_ITER):\n",
    "        print(f\"--- EM Iteration {i + 1}/{MAX_ITER} ---\")\n",
    "\n",
    "        # --- E-Step: Estimate labels for unlabeled data ---\n",
    "        if i == 0:\n",
    "            # Initial model trained only on labeled data\n",
    "            model = GaussianNB()\n",
    "            model.fit(X_labeled, y_labeled, sample_weight=base_weights)\n",
    "\n",
    "        # Predict probabilities for the unlabeled set\n",
    "        y_unlabeled_probs = model.predict_proba(X_unlabeled)\n",
    "        y_unlabeled_pred = model.classes_[np.argmax(y_unlabeled_probs, axis=1)]\n",
    "\n",
    "        # Monitor confidence/certainty of predictions\n",
    "        avg_confidence = np.mean(np.max(y_unlabeled_probs, axis=1))\n",
    "        print(f\"Average confidence on unlabeled data: {avg_confidence:.4f}\")\n",
    "\n",
    "        # --- M-Step: Retrain model with labeled and \"soft\"-labeled data ---\n",
    "        # Combine original labeled data with newly predicted labels for unlabeled data\n",
    "        X_combined = np.vstack((X_labeled, X_unlabeled))\n",
    "        y_combined = np.hstack((y_labeled, y_unlabeled_pred))\n",
    "\n",
    "        # Create sample weights. Labeled data has weight 1.\n",
    "        # Unlabeled data has weight equal to the probability of its predicted class.\n",
    "        unlabeled_weights = np.max(y_unlabeled_probs, axis=1)\n",
    "        sample_weights_combined = np.hstack((base_weights, unlabeled_weights))\n",
    "\n",
    "        # Retrain the model on the combined dataset with sample weights\n",
    "        model = GaussianNB()\n",
    "        model.fit(X_combined, y_combined, sample_weight=sample_weights_combined)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_accuracy = np.mean(y_val_pred == y_val)\n",
    "        print(f\"Validation Accuracy after iteration {i + 1}: {val_accuracy:.2%}\")\n",
    "\n",
    "        # Convergence based on prediction stability\n",
    "        if i > 0 and np.array_equal(y_unlabeled_pred, prev_predictions):\n",
    "            print(\"Convergence: predictions stabilized.\")\n",
    "            break\n",
    "        prev_predictions = y_unlabeled_pred.copy()\n",
    "\n",
    "    cv_scores.append(val_accuracy)\n",
    "    print(\n",
    "        f\"\\nEM training complete for dataset {dataset_idx}. Final CV Accuracy: {val_accuracy:.2%}\\n\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nOverall CV Accuracy across datasets: {np.mean(cv_scores):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
