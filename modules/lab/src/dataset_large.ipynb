{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bae0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-12 20:38:43.112136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760294323.192802    4402 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760294323.216713    4402 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-12 20:38:43.391637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Iterable\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from features import FeatureExtractorPipeline, ExtCtx\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from book_segmenting import TextSegmenter\n",
    "from utils import DATA_DIR\n",
    "\n",
    "feature_extractor = FeatureExtractorPipeline()\n",
    "\n",
    "SEGMENT_CHARS_MIN = 150\n",
    "SEGMENT_CHARS_MAX = 500\n",
    "segmenter = TextSegmenter(chunk_size=(SEGMENT_CHARS_MIN, SEGMENT_CHARS_MAX))\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    MIN_TEXT_LENGTH = 60\n",
    "    MAX_TEXT_LENGTH = 500\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        src: Iterable[Any],\n",
    "        take: int,\n",
    "        skip: int = 0,\n",
    "        text_getter=None,\n",
    "        deduplicate=False,\n",
    "        segment=False,\n",
    "        check_length=True,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.src = iter(src)\n",
    "        self.take = take\n",
    "        self.skip = skip\n",
    "        self.texts: list[str] | None = None\n",
    "        self.contexts: list[ExtCtx] | None = None\n",
    "        self.features: list[NDArray[np.float32]] | None = None\n",
    "        self.text_getter = text_getter\n",
    "        self.deduplicate = deduplicate\n",
    "        self.segment = segment\n",
    "        self.check_length = check_length\n",
    "\n",
    "    def process(\n",
    "        self, deduplicate: bool | None = None, segment: bool | None = None\n",
    "    ) -> list[ExtCtx]:\n",
    "        if deduplicate is None:\n",
    "            deduplicate = self.deduplicate\n",
    "        if segment is None:\n",
    "            segment = self.segment\n",
    "\n",
    "        self.texts = []\n",
    "        self.contexts = []\n",
    "        self.features = []\n",
    "        if deduplicate:\n",
    "            seen = set()\n",
    "        taken = 0\n",
    "        to_skip = self.skip\n",
    "\n",
    "        with tqdm(total=self.take, desc=\"Processing texts\", unit=\"text\") as pbar:\n",
    "            while taken < self.take:\n",
    "                try:\n",
    "                    text = next(self.src)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                if to_skip > 0:\n",
    "                    to_skip -= 1\n",
    "                    continue\n",
    "\n",
    "                if self.text_getter is not None:\n",
    "                    text = self.text_getter(text)\n",
    "                if not text or (\n",
    "                    self.check_length and len(text.strip()) < Dataset.MIN_TEXT_LENGTH\n",
    "                ):\n",
    "                    continue\n",
    "                if deduplicate:\n",
    "                    if text in seen:\n",
    "                        continue\n",
    "                    seen.add(text)\n",
    "\n",
    "                text = FeatureExtractorPipeline.preprocess(text)\n",
    "                if self.check_length and len(text.strip()) < Dataset.MIN_TEXT_LENGTH:\n",
    "                    continue\n",
    "\n",
    "                if segment:\n",
    "                    segments = [\n",
    "                        seg\n",
    "                        for seg in segmenter.segment_text(text)\n",
    "                        if seg\n",
    "                        and (\n",
    "                            not self.check_length\n",
    "                            or (seg_len := len(seg.strip())) >= Dataset.MIN_TEXT_LENGTH\n",
    "                            and seg_len <= Dataset.MAX_TEXT_LENGTH\n",
    "                        )\n",
    "                    ]\n",
    "                    if len(segments) == 0:\n",
    "                        continue\n",
    "                    example = segments[len(segments) // 2]\n",
    "                    ctx = feature_extractor.get_ctx(example)\n",
    "                    self.texts.append(example)\n",
    "                    self.contexts.append(ctx)\n",
    "                    self.features.append(\n",
    "                        feature_extractor.extract(example, preprocess=False, ctx=ctx)\n",
    "                    )\n",
    "                else:\n",
    "                    if self.check_length and len(text) > Dataset.MAX_TEXT_LENGTH:\n",
    "                        continue\n",
    "                    ctx = feature_extractor.get_ctx(text)\n",
    "                    self.texts.append(text)\n",
    "                    self.contexts.append(ctx)\n",
    "                    self.features.append(\n",
    "                        feature_extractor.extract(text, preprocess=False, ctx=ctx)\n",
    "                    )\n",
    "                taken += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        return self.contexts\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.contexts is None:\n",
    "            raise ValueError(\"Dataset not processed yet. Call process() first.\")\n",
    "        return iter(self.contexts)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.contexts is None:\n",
    "            raise ValueError(\"Dataset not processed yet. Call process() first.\")\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def save_as_parquet(self, labels: list[Any] | None = None):\n",
    "        if self.texts is None or self.features is None:\n",
    "            raise ValueError(\"Dataset not processed yet. Call process() first.\")\n",
    "        df = pd.DataFrame(\n",
    "            {\"text\": self.texts, \"features\": [feat.tolist() for feat in self.features]}\n",
    "        )\n",
    "        if labels is not None:\n",
    "            if len(labels) != len(self.texts):\n",
    "                raise ValueError(\"Labels length does not match texts length.\")\n",
    "            df[\"label\"] = labels\n",
    "        df.to_parquet(\n",
    "            DATA_DIR / \"datasets\" / \"large\" / f\"{self.name}.parquet\", index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a610ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_high_flickr = load_dataset(\n",
    "    \"CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "ds_flickr30k = load_dataset(\"embedding-data/flickr30k_captions_quintets\", split=\"train\")\n",
    "ds_coco = load_dataset(\"sentence-transformers/coco-captions\", split=\"train\")\n",
    "ds_sbu = load_dataset(\"vicenteor/sbu_captions\", split=\"train\", trust_remote_code=True)\n",
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"movie_summaries.txt\") as f:\n",
    "    ds_movie_summaries = [line.strip() for line in f.readlines()]\n",
    "ds_book_summaries = load_dataset(\"textminr/cmu-book-summaries\", split=\"train\")\n",
    "with open(DATA_DIR / \"datasets\" / \"large\" / \"book_dialogs.txt\") as f:\n",
    "    ds_book_dialogs = [line.strip() for line in f.read().split(\"\\n\\n\")]\n",
    "ds_wiki = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "ds_news = load_dataset(\"EdinburghNLP/xsum\", split=\"validation\")\n",
    "ds_hotels = load_dataset(\"argilla/tripadvisor-hotel-reviews\", split=\"train\")\n",
    "ds_yelp = load_dataset(\"Yelp/yelp_review_full\", split=\"test\")\n",
    "ds_arxiv = load_dataset(\n",
    "    \"armanc/scientific_papers\",\n",
    "    \"arxiv\",\n",
    "    split=\"validation\",\n",
    "    trust_remote_code=True,\n",
    "    streaming=True,\n",
    ")\n",
    "AMAZON_CATEGORIES = [\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Beauty_and_Personal_Care\",\n",
    "    \"Electronics\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"CDs_and_Vinyl\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Magazine_Subscriptions\",\n",
    "    \"Industrial_and_Scientific\",\n",
    "    \"Software\",\n",
    "]\n",
    "ds_amazon_reviews = []\n",
    "N_TOTAL = 15000\n",
    "for category in AMAZON_CATEGORIES:\n",
    "    ds = iter(\n",
    "        load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{category}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "            streaming=True,\n",
    "        )\n",
    "    )\n",
    "    for i in range(N_TOTAL // len(AMAZON_CATEGORIES)):\n",
    "        ds_amazon_reviews.append(next(ds))\n",
    "while len(ds_amazon_reviews) < N_TOTAL:\n",
    "    ds_amazon_reviews.append(next(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 0/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  35%|███▌      | 5316/15000 [10:11<17:31,  9.21text/s] /home/terra/Projects/vis-desc/modules/lab/src/features.py:547: RuntimeWarning: invalid value encountered in divide\n",
      "  return length_counts / total_ngrams\n",
      "Processing texts: 100%|██████████| 15000/15000 [25:33<00:00,  9.78text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 1/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 15000/15000 [14:14<00:00, 17.56text/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 2/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:  90%|█████████ | 13504/15000 [07:24<00:49, 30.41text/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 3/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [03:46<00:00, 22.06text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 4/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [03:36<00:00, 23.05text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 5/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [06:00<00:00, 13.86text/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 6/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [06:08<00:00, 13.57text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 7/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [08:31<00:00,  9.78text/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 8/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 10000/10000 [15:25<00:00, 10.81text/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 9/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [05:51<00:00, 14.23text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 10/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 2000/2000 [02:41<00:00, 12.39text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 11/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 3000/3000 [04:26<00:00, 11.25text/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 12/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [07:49<00:00, 10.65text/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done: 13/14 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████| 5000/5000 [06:10<00:00, 13.50text/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    Dataset(\n",
    "        \"artif_5\",\n",
    "        ds_high_flickr,\n",
    "        skip=2 * 1500,\n",
    "        take=15000,\n",
    "        text_getter=lambda x: x[\"caption_internlm2\"],\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"artif_4\",\n",
    "        ds_high_flickr,\n",
    "        skip=2 * 1500,\n",
    "        take=15000,\n",
    "        text_getter=lambda x: x[\"caption_internlm2_short\"],\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"flickr30k\",\n",
    "        ds_flickr30k,\n",
    "        skip=2 * 1500,\n",
    "        take=15000,\n",
    "        text_getter=lambda x: x[\"set\"][0],\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"coco\", ds_coco, skip=2 * 500, take=5000, text_getter=lambda x: x[\"caption1\"]\n",
    "    ),\n",
    "    Dataset(\"sbu\", ds_sbu, skip=2 * 500, take=5000, text_getter=lambda x: x[\"caption\"]),\n",
    "    Dataset(\n",
    "        \"movie_summaries\", ds_movie_summaries, skip=2 * 500, take=5000, segment=True\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"book_summaries\",\n",
    "        ds_book_summaries,\n",
    "        skip=2 * 500,\n",
    "        take=5000,\n",
    "        text_getter=lambda x: x[\"summary\"],\n",
    "        segment=True,\n",
    "    ),\n",
    "    Dataset(\"book_dialogs\", ds_book_dialogs, skip=2 * 500, take=5000),\n",
    "    Dataset(\n",
    "        \"wiki\",\n",
    "        ds_wiki,\n",
    "        skip=2 * 1000,\n",
    "        take=10000,\n",
    "        text_getter=lambda x: x[\"text\"].replace(\" @-@ \", \"-\").replace(\" @,@ \", \",\"),\n",
    "        segment=True,\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"news\",\n",
    "        ds_news,\n",
    "        skip=2 * 500,\n",
    "        take=5000,\n",
    "        text_getter=lambda x: x[\"document\"],\n",
    "        segment=True,\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"hotels\", ds_hotels, skip=2 * 200, take=2000, text_getter=lambda x: x[\"text\"]\n",
    "    ),\n",
    "    Dataset(\"yelp\", ds_yelp, skip=2 * 300, take=3000, text_getter=lambda x: x[\"text\"]),\n",
    "    Dataset(\n",
    "        \"arxiv\",\n",
    "        ds_arxiv,\n",
    "        skip=2 * 500,\n",
    "        take=5000,\n",
    "        text_getter=lambda x: x[\"abstract\"],\n",
    "        segment=True,\n",
    "    ),\n",
    "    Dataset(\n",
    "        \"amazon_reviews\",\n",
    "        ds_amazon_reviews,\n",
    "        skip=2 * 500,\n",
    "        take=5000,\n",
    "        text_getter=lambda x: x[\"text\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f\"--- Done: {i}/{len(datasets)} ---\")\n",
    "    dataset.process(deduplicate=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c26b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Fill the missing examples in ds_flickr30k with second caption from the set\n",
    "to_fill = 15000 - len(datasets[2].features)\n",
    "if to_fill > 0:\n",
    "    ds_flickr30k_2 = Dataset(\n",
    "        \"flickr30k\",\n",
    "        ds_flickr30k,\n",
    "        skip=2 * 1500,\n",
    "        take=to_fill,\n",
    "        text_getter=lambda x: x[\"set\"][1],\n",
    "    )\n",
    "    ds_flickr30k_2.process(deduplicate=True)\n",
    "    datasets[2].texts.extend(ds_flickr30k_2.texts)\n",
    "    datasets[2].contexts.extend(ds_flickr30k_2.contexts)\n",
    "    datasets[2].features.extend(ds_flickr30k_2.features)\n",
    "\n",
    "print(sum(len(ds.features) for ds in datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f891e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].save_as_parquet(labels=np.ones(len(datasets[0].features), dtype=int) * 5)\n",
    "datasets[1].save_as_parquet(labels=np.ones(len(datasets[1].features), dtype=int) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887336c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving 2 flickr30k 15000\n",
      "saving 13 amazon_reviews 5000\n",
      "saving 10 hotels 2000\n",
      "saving 8 wiki 10000\n",
      "saving 11 yelp 3000\n",
      "saving 4 sbu 5000\n",
      "saving 12 arxiv 5000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for path in (DATA_DIR / \"models\").glob(\"ordinal_model_dataset_*.pkl\"):\n",
    "    with open(path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    # Use the model for classification\n",
    "    dataset_idx = int(path.stem.split(\"_\")[-1])\n",
    "    X = np.array(datasets[dataset_idx].features)\n",
    "    preds = model.predict(X)\n",
    "    # Save\n",
    "    print(\"saving\", dataset_idx, datasets[dataset_idx].name, len(preds))\n",
    "    datasets[dataset_idx].save_as_parquet(labels=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5ad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      "label\n",
      "0    26442\n",
      "1    15519\n",
      "2     4509\n",
      "3    20808\n",
      "4    17722\n",
      "5    15000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from utils import DATA_DIR\n",
    "\n",
    "# Combine the individual parquet files into a single one\n",
    "parquet_files = list((DATA_DIR / \"datasets\" / \"large\").glob(\"*.parquet\"))\n",
    "dfs = []\n",
    "for p in parquet_files:\n",
    "    name = p.stem\n",
    "    df = pd.read_parquet(p)\n",
    "    df[\"dataset\"] = name\n",
    "    dfs.append(df)\n",
    "\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "df_combined.to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"combined.parquet\", index=False\n",
    ")\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "print(df_combined[\"label\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2ab36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER\n",
    "# Delete the individual parquet files except combined.parquet\n",
    "for p in parquet_files:\n",
    "    if p.stem != \"combined\":\n",
    "        p.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
