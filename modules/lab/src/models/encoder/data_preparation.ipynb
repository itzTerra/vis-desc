{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fab5090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/Projects/vis-desc/modules/lab/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from models.encoder.common import SEED\n",
    "from utils import DATA_DIR\n",
    "\n",
    "ds_large = pd.read_parquet(DATA_DIR / \"datasets\" / \"large\" / \"combined.parquet\")\n",
    "ds_small = pd.read_parquet(DATA_DIR / \"datasets\" / \"small\" / \"agreed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts before balancing:\n",
      "label\n",
      "3    131\n",
      "0    122\n",
      "2    113\n",
      "1     70\n",
      "4     58\n",
      "5      6\n",
      "Name: count, dtype: int64\n",
      "Added 51 samples for label 4\n",
      "Added 103 samples for label 5\n",
      "Final small dataset size: 654\n",
      "Label counts after balancing:\n",
      "label\n",
      "3    131\n",
      "0    122\n",
      "2    113\n",
      "4    109\n",
      "5    109\n",
      "1     70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill labels 4 and 5 to atleast average number of samples per class for the other classes from large dataset\n",
    "label_counts = ds_small[\"label\"].value_counts()\n",
    "print(f\"Label counts before balancing:\\n{label_counts}\")\n",
    "\n",
    "target_labels = [4, 5]\n",
    "other_counts = label_counts.loc[~label_counts.index.isin(target_labels)]\n",
    "avg_count = int(other_counts.mean())\n",
    "for label in [4, 5]:\n",
    "    current_count = label_counts[label]\n",
    "    added = 0\n",
    "    if current_count < avg_count:\n",
    "        needed = avg_count - current_count\n",
    "        samples_to_add = ds_large[ds_large[\"label\"] == label].sample(\n",
    "            n=needed, random_state=SEED\n",
    "        )\n",
    "        ds_small = pd.concat([ds_small, samples_to_add], ignore_index=True)\n",
    "        added += needed\n",
    "    print(f\"Added {added} samples for label {label}\")\n",
    "\n",
    "print(f\"Final small dataset size: {len(ds_small)}\")\n",
    "label_counts = ds_small[\"label\"].value_counts()\n",
    "print(f\"Label counts after balancing:\\n{label_counts}\")\n",
    "\n",
    "# Ensure the 'features' column has a consistent numeric dtype across all rows to avoid\n",
    "# pyarrow/arrow errors when writing to parquet (mixing float32 and float64).\n",
    "# Convert each features entry to a list of Python floats with a fixed dtype.\n",
    "ds_small[\"features\"] = ds_small[\"features\"].apply(\n",
    "    lambda arr: np.asarray(arr, dtype=np.float32).astype(float).tolist()\n",
    ")\n",
    "\n",
    "ds_small.to_parquet(DATA_DIR / \"datasets\" / \"small\" / \"balanced.parquet\", index=True)\n",
    "\n",
    "sm_train, sm_test = train_test_split(\n",
    "    ds_small, test_size=0.4, random_state=SEED, stratify=ds_small[\"label\"]\n",
    ")\n",
    "pd.DataFrame(sm_train).to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"small\" / \"train.parquet\", index=True\n",
    ")\n",
    "pd.DataFrame(sm_test).to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"small\" / \"test.parquet\", index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1080eb2",
   "metadata": {},
   "source": [
    "### ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c12f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 90898.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from text2features import FeatureService\n",
    "from text2features_paths import (\n",
    "    FEATURE_PIPELINE_RESOURCES,\n",
    ")\n",
    "from sklearn.utils import gen_batches\n",
    "\n",
    "feature_service = FeatureService(\n",
    "    feature_pipeline_resources=FEATURE_PIPELINE_RESOURCES,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96003714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 21it [05:35, 15.99s/it]\n"
     ]
    }
   ],
   "source": [
    "batches = gen_batches(len(ds_small), BATCH_SIZE)\n",
    "embedding_series = []\n",
    "\n",
    "for batch in tqdm(batches, desc=\"Extracting embeddings\"):\n",
    "    texts = ds_small[\"text\"].iloc[batch].tolist()\n",
    "    features = feature_service.get_modernbert_embeddings(texts)\n",
    "    embedding_series.extend(features)\n",
    "\n",
    "embedding_df = ds_small[[\"text\"]].copy()\n",
    "embedding_df[\"cls\"] = embedding_series\n",
    "embedding_df.to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"small\" / \"modernbert_cls_embeddings.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c2fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 3125it [6:43:54,  7.76s/it]\n"
     ]
    }
   ],
   "source": [
    "batches = gen_batches(len(ds_large), BATCH_SIZE)\n",
    "embedding_series = []\n",
    "\n",
    "for batch in tqdm(batches, desc=\"Extracting embeddings\"):\n",
    "    texts = ds_large[\"text\"].iloc[batch].tolist()\n",
    "    features = feature_service.get_modernbert_embeddings(texts)\n",
    "    embedding_series.extend(features)\n",
    "\n",
    "embedding_df = ds_large[[\"text\"]].copy()\n",
    "embedding_df[\"cls\"] = embedding_series\n",
    "embedding_df.to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"modernbert_cls_embeddings.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79749009",
   "metadata": {},
   "source": [
    "### MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055fc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 11:48:10.298589: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763376490.399135  723760 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763376490.427716  723760 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-17 11:48:10.661105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e147954",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = ds_small[[\"text\"]].copy()\n",
    "embedding_df[\"cls\"] = model.encode(ds_small[\"text\"].tolist()).tolist()\n",
    "embedding_df.to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"small\" / \"minilm_embeddings.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = ds_large[[\"text\"]].copy()\n",
    "embedding_df[\"cls\"] = model.encode(ds_large[\"text\"].tolist()).tolist()\n",
    "embedding_df.to_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"minilm_embeddings.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6d706",
   "metadata": {},
   "source": [
    "### Validate saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from utils import DATA_DIR\n",
    "\n",
    "sm_minilm = pd.read_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"small\" / \"minilm_embeddings.parquet\"\n",
    ")\n",
    "sm_modernbert = pd.read_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"small\" / \"modernbert_cls_embeddings.parquet\"\n",
    ")\n",
    "\n",
    "ds_small = pd.read_parquet(DATA_DIR / \"datasets\" / \"small\" / \"balanced.parquet\")\n",
    "SM_LENGTH = len(ds_small)\n",
    "assert len(sm_minilm) == SM_LENGTH, f\"{len(sm_minilm)} != {SM_LENGTH}\"\n",
    "assert len(sm_modernbert) == SM_LENGTH, f\"{len(sm_modernbert)} != {SM_LENGTH}\"\n",
    "\n",
    "# Duplicates in 'text' column\n",
    "assert sm_minilm[\"text\"].nunique() == SM_LENGTH\n",
    "assert sm_minilm[\"cls\"].apply(tuple).nunique() == SM_LENGTH\n",
    "assert sm_modernbert[\"text\"].nunique() == SM_LENGTH\n",
    "assert sm_modernbert[\"cls\"].apply(tuple).nunique() == SM_LENGTH\n",
    "\n",
    "# NaNs in 'cls' lists\n",
    "assert not sm_minilm.explode(\"cls\")[\"cls\"].isna().any()\n",
    "assert not sm_modernbert.explode(\"cls\")[\"cls\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6b3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from utils import DATA_DIR\n",
    "\n",
    "lg_minilm = pd.read_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"minilm_embeddings.parquet\"\n",
    ")\n",
    "lg_modernbert = pd.read_parquet(\n",
    "    DATA_DIR / \"datasets\" / \"large\" / \"modernbert_cls_embeddings.parquet\"\n",
    ")\n",
    "\n",
    "LG_LENGTH = 100000\n",
    "\n",
    "assert len(lg_minilm) == LG_LENGTH, f\"{len(lg_minilm)} != {LG_LENGTH}\"\n",
    "assert len(lg_modernbert) == LG_LENGTH, f\"{len(lg_modernbert)} != {LG_LENGTH}\"\n",
    "\n",
    "# Duplicates in 'text' column\n",
    "assert lg_minilm[\"text\"].nunique() == LG_LENGTH\n",
    "assert lg_minilm[\"cls\"].apply(tuple).nunique() == LG_LENGTH\n",
    "\n",
    "assert lg_modernbert[\"text\"].nunique() == LG_LENGTH\n",
    "assert lg_modernbert[\"cls\"].apply(tuple).nunique() == LG_LENGTH\n",
    "\n",
    "# NaNs in 'cls' lists\n",
    "assert not lg_minilm.explode(\"cls\")[\"cls\"].isna().any()\n",
    "assert not lg_modernbert.explode(\"cls\")[\"cls\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1062b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(len(lg_modernbert[\"cls\"].iloc[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
